{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60c77fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1755726959.0\n"
     ]
    }
   ],
   "source": [
    "import pyreadr\n",
    "transactions_df = pyreadr.read_r(\"/home/spadef/data/IDEA_DeFi_Research/Data/Lending_Protocols/Aave/V3/Polygon/transactionsFinSurvival.rds\")[None]\n",
    "# transactions_df.to_csv(\"/home/spadef/data/IDEA_DeFi_Research/Data/Lending_Protocols/Aave/V3/Polygon/transactionsFinSurvival.csv\", index=False)\n",
    "print(transactions_df[\"timestamp\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74406a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "EVENTS = [\"Deposit\", \"Withdraw\", \"Repay\", \"Borrow\", \"Liquidated\"]\n",
    "DATA_PATH = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4231de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = {}\n",
    "for index_event in EVENTS:\n",
    "    for outcome_event in EVENTS:\n",
    "        csv_path = os.path.join(DATA_PATH, index_event, outcome_event, \"data.csv\")\n",
    "        if not os.path.exists(csv_path):\n",
    "            continue\n",
    "        df = pd.read_csv(csv_path)\n",
    "        for _, row in df.iterrows():\n",
    "            if row[\"user\"] not in users:\n",
    "                users[row[\"user\"]] = {\n",
    "                    \"user_address\": \"0xUser123\",\n",
    "                    \"description\": \"My custom user profile\",\n",
    "                    \"transactions\": [],\n",
    "                }\n",
    "            users[row[\"user\"]][\"transactions\"].append(\n",
    "                {\"action\": row[\"\"], \"symbol\": \"USDC\", \"amount\": 5000, \"timestamp\": 0}\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9952a589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated ./data/Deposit/Withdraw/data.csv\n"
     ]
    }
   ],
   "source": [
    "for index_event in EVENTS:\n",
    "    for outcome_event in EVENTS:\n",
    "        if not (index_event == \"Deposit\" and outcome_event == \"Withdraw\"):\n",
    "            continue\n",
    "        csv_path = os.path.join(DATA_PATH, index_event, outcome_event, \"data.csv\")\n",
    "        if not os.path.exists(csv_path):\n",
    "            continue\n",
    "        df = pd.read_csv(csv_path)\n",
    "        for col in [\"Index Event\", \"Outcome Event\"]:\n",
    "            if col in df.columns:\n",
    "                mask = df[col].notna()\n",
    "                df.loc[mask, col] = (\n",
    "                    df.loc[mask, col]\n",
    "                    .astype(str)\n",
    "                    .str.replace(\"account liquidated\", \"liquidated\", case=False, regex=False)\n",
    "                    .str.strip()\n",
    "                )\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"Updated {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "614e5e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated ./data/Deposit/Withdraw/data.csv\n"
     ]
    }
   ],
   "source": [
    "for index_event in EVENTS:\n",
    "    for outcome_event in EVENTS:\n",
    "        if not (index_event == \"Deposit\" and outcome_event == \"Withdraw\"):\n",
    "            continue\n",
    "        csv_path = os.path.join(DATA_PATH, index_event, outcome_event, \"data.csv\")\n",
    "        if not os.path.exists(csv_path):\n",
    "            continue\n",
    "        df = pd.read_csv(csv_path)\n",
    "        for col in [\"Index Event\", \"Outcome Event\"]:\n",
    "            if col in df.columns:\n",
    "                mask = df[col].notna()\n",
    "                df.loc[mask, col] = (\n",
    "                    df.loc[mask, col]\n",
    "                    .astype(str)\n",
    "                    .str.replace(\"liquidation\", \"liquidated\", case=False, regex=False)\n",
    "                    .str.strip()\n",
    "                )\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"Updated {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd7becd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in EVENTS:\n",
    "    csv_path = os.path.join(DATA_PATH, event, event, \"data.csv\")\n",
    "    if not (event == \"Deposit\" and event == \"Withdraw\"):\n",
    "        continue\n",
    "    if not os.path.exists(csv_path):\n",
    "        continue\n",
    "    df = pd.read_csv(csv_path)\n",
    "    for col in [\"Outcome Event\"]:\n",
    "        if col in df.columns:\n",
    "            mask = df[col].notna()\n",
    "            df.loc[mask, col] = (\n",
    "                df.loc[mask, col]\n",
    "                .astype(str)\n",
    "                .str.replace(event + \"_outcome\", event, case=False, regex=False)\n",
    "                .str.strip()\n",
    "            )\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Updated {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e348324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in EVENTS:\n",
    "    csv_path = os.path.join(DATA_PATH, event, event, \"data.csv\")\n",
    "    if not (event == \"Deposit\" and event == \"Withdraw\"):\n",
    "        continue\n",
    "    if not os.path.exists(csv_path):\n",
    "        continue\n",
    "    df = pd.read_csv(csv_path)\n",
    "    for col in [\"Outcome Event\"]:\n",
    "        if col in df.columns:\n",
    "            mask = df[col].notna()\n",
    "            df.loc[mask, col] = (\n",
    "                df.loc[mask, col]\n",
    "                .astype(str)\n",
    "                .str.replace(event, event.lower(), case=False, regex=False)\n",
    "                .str.strip()\n",
    "            )\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Updated {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0100842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated ./data/Liquidated/Deposit/data.csv\n",
      "Updated ./data/Liquidated/Withdraw/data.csv\n",
      "Updated ./data/Liquidated/Repay/data.csv\n",
      "Updated ./data/Liquidated/Borrow/data.csv\n",
      "Updated ./data/Liquidated/Liquidated/data.csv\n"
     ]
    }
   ],
   "source": [
    "index_event = \"Liquidated\"\n",
    "for outcome_event in EVENTS:\n",
    "    csv_path = os.path.join(DATA_PATH, index_event, outcome_event, \"data.csv\")\n",
    "    if not os.path.exists(csv_path):\n",
    "        continue\n",
    "    df = pd.read_csv(csv_path)\n",
    "    for col in [\"Index Event\", \"Outcome Event\"]:\n",
    "        if col in df.columns:\n",
    "            mask = df[col].notna()\n",
    "            df.loc[mask, col] = (\n",
    "                df.loc[mask, col]\n",
    "                .astype(str)\n",
    "                .str.replace(\"liquidation\", \"liquidated\", case=False, regex=False)\n",
    "                .str.strip()\n",
    "            )\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Updated {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3ab60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WETH\n",
      "0xae0cc724fc85cd394435eb4f9ddfd3a8c088b4e2's\"Debt\" before liquidation: ['USDC: 898.7291915312042', 'WETH: -1.6900000000003718e-06']\n",
      "USDC\n",
      "0x0e7b3aea0428137786edf20617670179435c530b's\"Debt\" before liquidation: ['USDC: 0.0', 'WETH: 75.7625662202381']\n",
      "USDC\n",
      "0x97dc47d17ab56c32a4d2da1839e3ff261304e95d's\"Debt\" before liquidation: ['USDC: 7.854323']\n",
      "WETH\n",
      "0x5091d57b4feb57db1da04b51cc5454f371b623a6's\"Debt\" before liquidation: ['USDC: 0.0']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "usersToTest = [\n",
    "    \"0xae0cc724fc85cd394435eb4f9ddfd3a8c088b4e2\",\n",
    "    \"0x0e7b3aea0428137786edf20617670179435c530b\",\n",
    "    \"0x97dc47d17ab56c32a4d2da1839e3ff261304e95d\",\n",
    "    \"0x5091d57b4feb57db1da04b51cc5454f371b623a6\",\n",
    "]\n",
    "\n",
    "for user in usersToTest:\n",
    "    with open(\n",
    "        f\"../data/IDEA_DeFi_Research/Data/CSV/profile-generation/user_{user}.json\"\n",
    "    ) as f:\n",
    "        userTest = json.load(f)\n",
    "\n",
    "    userTransactions = userTest[\"transactions\"]\n",
    "    print(next(\n",
    "            (e for i, e in enumerate(userTransactions) if (e[\"action\"] == \"Liquidated\"))\n",
    "        )['debt_symbol'])\n",
    "    userTransactions = userTransactions[\n",
    "        : next(\n",
    "            (i for i, e in enumerate(userTransactions) if (e[\"action\"] == \"Liquidated\")), -1\n",
    "        )\n",
    "    ]\n",
    "    borrows = defaultdict(int)\n",
    "    repays = defaultdict(int)\n",
    "    for transaction in userTransactions:\n",
    "        if transaction[\"action\"] == \"Borrow\":\n",
    "            borrows[transaction[\"symbol\"]] += transaction[\"amount\"]\n",
    "        elif transaction[\"action\"] == \"Repay\":\n",
    "            repays[transaction[\"symbol\"]] += transaction[\"amount\"]\n",
    "    print(f'{user}\\'s \"Debt\" before liquidation: {[f'{symbol}: {borrows[symbol] - repays[symbol]}' for symbol in borrows.keys()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63ed1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0x0000000000000000000000000000000000000001\n",
       "1    0x0000000000000000000000000000000000001010\n",
       "2    0x000000000000000000000000000000000000dead\n",
       "3    0x0000000000085a12481aedb59eb3200332aca597\n",
       "4    0x00000000000a29a0800f6f557ddbbe8249397de7\n",
       "Name: id, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open('../data/IDEA_DeFi_Research/Data/Lending_Protocols/Aave/V2/Polygon/users.csv') as f:\n",
    "    old_users = pd.read_csv(f)['id']\n",
    "old_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bf27bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 365677 users, 308205 users were not among the 1121518 old users.\n"
     ]
    }
   ],
   "source": [
    "with open('./data/users.csv') as f:\n",
    "    users = pd.read_csv(f)['id']\n",
    "new_users = set(users) - set(old_users)\n",
    "print(f'Out of {len(users)} users, {len(new_users)} users were not among the {len(old_users)} old users.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19763351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got liquidated users from ./data/Deposit/Liquidated/data.csv\n",
      "Got liquidated users from ./data/Withdraw/Liquidated/data.csv\n",
      "Got liquidated users from ./data/Repay/Liquidated/data.csv\n",
      "Got liquidated users from ./data/Borrow/Liquidated/data.csv\n",
      "Out of 259775 liquidated users, 204309 liquidated users were not among the 1121518 old users.\n"
     ]
    }
   ],
   "source": [
    "usersWithLiquidation = set()\n",
    "outcome_event = \"Liquidated\"\n",
    "for index_event in EVENTS:\n",
    "    if index_event == outcome_event:\n",
    "        continue\n",
    "    csv_path = os.path.join(DATA_PATH, index_event, outcome_event, \"data.csv\")\n",
    "    if not os.path.exists(csv_path):\n",
    "        continue\n",
    "    df = pd.read_csv(csv_path)\n",
    "    usersWithLiquidation = usersWithLiquidation.union(set(df['user']))\n",
    "    print(f\"Got liquidated users from {csv_path}\")\n",
    "new_liquidated_users = set(usersWithLiquidation) - set(old_users)\n",
    "print(f'Out of {len(usersWithLiquidation)} liquidated users, {len(new_liquidated_users)} liquidated users were not among the {len(old_users)} old users.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d95345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got liquidated users from ./data/Liquidated/Deposit/data.csv\n",
      "Got liquidated users from ./data/Liquidated/Withdraw/data.csv\n",
      "Got liquidated users from ./data/Liquidated/Repay/data.csv\n",
      "Got liquidated users from ./data/Liquidated/Borrow/data.csv\n",
      "Out of 13743 liquidated users, 8899 liquidated users were not among the 1121518 old users.\n"
     ]
    }
   ],
   "source": [
    "usersWithLiquidation = set()\n",
    "index_event = \"Liquidated\"\n",
    "for outcome_event in EVENTS:\n",
    "    if index_event == outcome_event:\n",
    "        continue\n",
    "    csv_path = os.path.join(DATA_PATH, index_event, outcome_event, \"data.csv\")\n",
    "    if not os.path.exists(csv_path):\n",
    "        continue\n",
    "    df = pd.read_csv(csv_path)\n",
    "    usersWithLiquidation = usersWithLiquidation.union(set(df['user']))\n",
    "    print(f\"Got liquidated users from {csv_path}\")\n",
    "new_liquidated_users = set(usersWithLiquidation) - set(old_users)\n",
    "print(f'Out of {len(usersWithLiquidation)} liquidated users, {len(new_liquidated_users)} liquidated users were not among the {len(old_users)} old users.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65f22014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0x97dc47d17ab56c32a4d2da1839e3ff261304e95d',\n",
       " '0x5091d57b4feb57db1da04b51cc5454f371b623a6']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[user for user in usersToTest if user in new_liquidated_users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3956641c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 1328 zero-debt liquidated users, 366 zero-debt liquidated users were not among the 1121518 old users.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('./Aave-Simulator/results/zero-debt-liquidation-analysis/analysis_20251218_164135.json') as f:\n",
    "    zero_debt_liquidated_users = [case['user_address'] for case in json.load(f)['zero_debt_cases']]\n",
    "new_zero_liquidated_users = set(zero_debt_liquidated_users) - set(old_users)\n",
    "print(f'Out of {len(zero_debt_liquidated_users)} zero-debt liquidated users, {len(new_zero_liquidated_users)} zero-debt liquidated users were not among the {len(old_users)} old users.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85fc1f86",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mactionAgentTraining\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m verify_amount_feature_effect, get_model_for_pair_and_date\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m model = get_model_for_pair_and_date(\u001b[33m\"\u001b[39m\u001b[33mRepay\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mLiquidated\u001b[39m\u001b[33m\"\u001b[39m, pd.to_datetime(\u001b[33m\"\u001b[39m\u001b[33m2024-05-05 15:33:51.800000\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/actionAgent/actionAgentTraining.py:1214\u001b[39m\n\u001b[32m   1209\u001b[39m         logger.info(\u001b[33m\"\u001b[39m\u001b[33mRecommended \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mfor \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(recommendations[rowID]), rowID)\n\u001b[32m   1210\u001b[39m     \u001b[38;5;66;03m# with open(recommendation_cache_file, \"wb\") as f:\u001b[39;00m\n\u001b[32m   1211\u001b[39m     \u001b[38;5;66;03m#     pkl.dump(recommendations, f)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1214\u001b[39m \u001b[43mrun_training_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/actionAgent/actionAgentTraining.py:1205\u001b[39m, in \u001b[36mrun_training_pipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1203\u001b[39m rowID = \u001b[38;5;28mstr\u001b[39m(row)\n\u001b[32m   1204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rowID \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recommendations:\n\u001b[32m-> \u001b[39m\u001b[32m1205\u001b[39m     recommendations[rowID] = \u001b[43mrecommend_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1206\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m iter_count % \u001b[32m1\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m   1207\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(recommendation_cache_file, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/actionAgent/actionAgentTraining.py:1018\u001b[39m, in \u001b[36mrecommend_action\u001b[39m\u001b[34m(row)\u001b[39m\n\u001b[32m   1009\u001b[39m is_at_risk, most_recent_predictions, _ = determine_liquidation_risk(row)\n\u001b[32m   1011\u001b[39m recommended_action = (\n\u001b[32m   1012\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mRepay\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1013\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m most_recent_predictions[\u001b[33m\"\u001b[39m\u001b[33mRepay\u001b[39m\u001b[33m\"\u001b[39m] >= most_recent_predictions[\u001b[33m\"\u001b[39m\u001b[33mDeposit\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1014\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m is_at_risk\n\u001b[32m   1015\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mDeposit\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1016\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1018\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimize_recommendation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecommended_action\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/actionAgent/actionAgentTraining.py:935\u001b[39m, in \u001b[36moptimize_recommendation\u001b[39m\u001b[34m(row, recommended_action)\u001b[39m\n\u001b[32m    932\u001b[39m     logger.exception(\u001b[33m\"\u001b[39m\u001b[33mDEBUG: error preparing single-action prediction\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    934\u001b[39m \u001b[38;5;66;03m# If risk remains, iteratively increase the amount and log single-action predictions\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m935\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43mdetermine_liquidation_risk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_action\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]:\n\u001b[32m    936\u001b[39m     new_action = generate_next_transaction(\n\u001b[32m    937\u001b[39m         row,\n\u001b[32m    938\u001b[39m         recommended_action,\n\u001b[32m    939\u001b[39m         amount=new_action[\u001b[33m\"\u001b[39m\u001b[33mamount\u001b[39m\u001b[33m\"\u001b[39m] * \u001b[32m2\u001b[39m,\n\u001b[32m    940\u001b[39m     )\n\u001b[32m    941\u001b[39m     \u001b[38;5;66;03m# logger = logging.getLogger(__name__)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/actionAgent/actionAgentTraining.py:716\u001b[39m, in \u001b[36mdetermine_liquidation_risk\u001b[39m\u001b[34m(row)\u001b[39m\n\u001b[32m    713\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdetermine_liquidation_risk\u001b[39m(row: pd.Series):\n\u001b[32m    714\u001b[39m     predict_transaction_history = {\n\u001b[32m    715\u001b[39m         key: value\n\u001b[32m--> \u001b[39m\u001b[32m716\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[43mget_transaction_history_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m.items()\n\u001b[32m    717\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m value\n\u001b[32m    718\u001b[39m     }\n\u001b[32m    720\u001b[39m     is_at_risk = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    722\u001b[39m     most_recent_predictions = predict_transaction_history[\n\u001b[32m    723\u001b[39m         \u001b[38;5;28mmax\u001b[39m(predict_transaction_history.keys())\n\u001b[32m    724\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/actionAgent/actionAgentTraining.py:505\u001b[39m, in \u001b[36mget_transaction_history_predictions\u001b[39m\u001b[34m(row)\u001b[39m\n\u001b[32m    502\u001b[39m train_dates, test_dates = get_date_ranges()\n\u001b[32m    503\u001b[39m dates = train_dates.union(test_dates)\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m user_history = \u001b[43mget_user_history\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mup_to_timestamp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtimestamp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m    507\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    508\u001b[39m user_history = pd.concat([user_history, row.to_frame().T]).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    510\u001b[39m model_date = dates[dates <= pd.to_datetime(row[\u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m], unit=\u001b[33m\"\u001b[39m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)].max()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/actionAgent/actionAgentTraining.py:447\u001b[39m, in \u001b[36mget_user_history\u001b[39m\u001b[34m(user_id, up_to_timestamp)\u001b[39m\n\u001b[32m    445\u001b[39m event_path = os.path.join(DATA_PATH, index_event, outcome_event, \u001b[33m\"\u001b[39m\u001b[33mdata.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    446\u001b[39m \u001b[38;5;66;03m# use cached loader\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m event_df = \u001b[43mget_event_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_event\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutcome_event\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (event_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    449\u001b[39m     user_events = event_df[\n\u001b[32m    450\u001b[39m         (event_df[\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m] == user_id)\n\u001b[32m    451\u001b[39m         & (event_df[\u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m] <= up_to_timestamp)\n\u001b[32m    452\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/actionAgent/actionAgentTraining.py:81\u001b[39m, in \u001b[36mget_event_df\u001b[39m\u001b[34m(index_event, outcome_event)\u001b[39m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     83\u001b[39m     logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWarning: failed to read \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/finsur/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/finsur/lib/python3.13/site-packages/pandas/io/parsers/readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/finsur/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/finsur/lib/python3.13/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:334\u001b[39m, in \u001b[36mgetstate\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from actionAgentTraining import verify_amount_feature_effect, get_model_for_pair_and_date\n",
    "import pandas as pd\n",
    "\n",
    "model = get_model_for_pair_and_date(\"Repay\", \"Liquidated\", pd.to_datetime(\"2024-05-05 15:33:51.800000\"))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca423488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finsur",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

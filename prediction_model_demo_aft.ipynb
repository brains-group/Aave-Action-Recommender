{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FinSurvival Competition: Starter Notebook (XGBoost Cox Model Prediction Submission)\n",
    "\n",
    "**Objective:** This notebook provides a workflow for creating a valid prediction submission using the XGBoost Cox survival model. The competition requires you to submit a `.zip` file containing 16 separate prediction files in CSV format.\n",
    "\n",
    "This notebook will guide you through:\n",
    "1.  Loading the training and test sets for each of the 16 tasks from a single directory.\n",
    "2.  Training a model (using XGBoost Cox model as an example).\n",
    "3.  Generating predictions on the test set in the required format.\n",
    "4.  Saving each set of predictions to a correctly named CSV file.\n",
    "5.  Zipping all 16 prediction files for submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=1\n",
    "\n",
    "# Install required packages\n",
    "# pip install -q pandas xgboost scikit-learn numpy\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Tuple, Optional\n",
    "import pickle as pkl\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data/\"\n",
    "CACHE_DIR = \"./cache/\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "MODEL_CACHE_DIR = os.path.join(CACHE_DIR, \"models\")\n",
    "os.makedirs(MODEL_CACHE_DIR, exist_ok=True)\n",
    "DATA_CACHE_DIR = os.path.join(CACHE_DIR, \"data\")\n",
    "os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define a Preprocessing Function\n",
    "\n",
    "Even though you are not submitting this code, you will still need a preprocessing pipeline to train your models effectively. You can use the one below as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    train_df_with_labels: Optional[pd.DataFrame] = None,\n",
    "    test_features_df: Optional[pd.DataFrame] = None,\n",
    "    model_date: Optional[int] = None,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "\n",
    "    # Create unique prefix for saving/loading preprocessing objects\n",
    "    unique_prefix = (\n",
    "        (\n",
    "            present_dataframe := (\n",
    "                train_df_with_labels\n",
    "                if train_df_with_labels is not None\n",
    "                else test_features_df\n",
    "            )\n",
    "        )[\"Index Event\"].iloc[0]\n",
    "        + \"_\"\n",
    "        + present_dataframe[\"Outcome Event\"].iloc[0]\n",
    "        + (f\"_{model_date}_\" if model_date is not None else \"_\")\n",
    "    )\n",
    "    # Define paths for saving/loading preprocessing objects\n",
    "    scaler_path = os.path.join(DATA_CACHE_DIR, unique_prefix + \"scaler.pkl\")\n",
    "    train_cols = os.path.join(DATA_CACHE_DIR, unique_prefix + \"train_cols.pkl\")\n",
    "    top_categories_dict_path = os.path.join(\n",
    "        DATA_CACHE_DIR, unique_prefix + \"top_categories_dict.pkl\"\n",
    "    )\n",
    "\n",
    "    if train_df_with_labels is not None:\n",
    "        if model_date is not None:\n",
    "            train_df_with_labels = train_df_with_labels[\n",
    "                (train_df_with_labels[\"timestamp\"] + train_df_with_labels[\"timeDiff\"])\n",
    "                <= model_date\n",
    "            ]\n",
    "\n",
    "        # Separate features and targets (and drop unneeded columns from features)\n",
    "        target_columns = [\"timeDiff\", \"status\"]\n",
    "        train_targets = train_df_with_labels[target_columns]\n",
    "        cols_to_drop = [\n",
    "            \"id\",\n",
    "            \"user\",\n",
    "            \"pool\",\n",
    "            \"Index Event\",\n",
    "            \"Outcome Event\",\n",
    "            \"type\",\n",
    "            \"timestamp\",\n",
    "        ]\n",
    "        train_features = train_df_with_labels.drop(\n",
    "            columns=target_columns + cols_to_drop, errors=\"ignore\"\n",
    "        )\n",
    "\n",
    "        # Make uncommon categories \"Other\" and one-hot encode categorical features\n",
    "        categorical_cols = train_features.select_dtypes(\n",
    "            include=[\"object\", \"category\"]\n",
    "        ).columns\n",
    "        top_categories_dict = {}\n",
    "        for col in categorical_cols:\n",
    "            if col not in top_categories_dict:\n",
    "                top_categories_dict[col] = (\n",
    "                    train_features[col].value_counts().nlargest(10).index\n",
    "                )\n",
    "            train_features[col] = train_features[col].where(\n",
    "                train_features[col].isin(top_categories_dict[col]), \"Other\"\n",
    "            )\n",
    "        train_features_encoded = pd.get_dummies(\n",
    "            train_features, columns=categorical_cols, dummy_na=True, drop_first=True\n",
    "        )\n",
    "\n",
    "        # Standardize numerical features\n",
    "        numerical_cols = train_features_encoded.select_dtypes(include=np.number).columns\n",
    "        scaler = StandardScaler()\n",
    "        train_features_scaled = scaler.fit_transform(\n",
    "            train_features_encoded[numerical_cols]\n",
    "        )\n",
    "        train_features_final = pd.DataFrame(\n",
    "            train_features_scaled,\n",
    "            index=train_features_encoded.index,\n",
    "            columns=numerical_cols,\n",
    "        ).fillna(0)\n",
    "\n",
    "        # Remove zero-variance columns\n",
    "        cols_to_keep = train_features_final.columns[train_features_final.var() != 0]\n",
    "        train_features_final = train_features_final[cols_to_keep]\n",
    "\n",
    "        # Save preprocessing objects\n",
    "        with open(scaler_path, \"wb\") as f:\n",
    "            pkl.dump(scaler, f)\n",
    "        with open(train_cols, \"wb\") as f:\n",
    "            pkl.dump(train_features_encoded.columns, f)\n",
    "        with open(top_categories_dict_path, \"wb\") as f:\n",
    "            pkl.dump(top_categories_dict, f)\n",
    "\n",
    "    # Process test features if provided\n",
    "    test_processed_features = None\n",
    "    if test_features_df is not None:\n",
    "        test_features = test_features_df.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "        with open(top_categories_dict_path, \"rb\") as f:\n",
    "            top_categories_dict = pkl.load(f)\n",
    "        for col in categorical_cols:\n",
    "            top_categories = top_categories_dict[col]\n",
    "            test_features[col] = test_features[col].where(\n",
    "                test_features[col].isin(top_categories), \"Other\"\n",
    "            )\n",
    "        test_features_encoded = pd.get_dummies(\n",
    "            test_features, columns=categorical_cols, dummy_na=True, drop_first=True\n",
    "        )\n",
    "        with open(train_cols, \"rb\") as f:\n",
    "            train_cols = pkl.load(f)\n",
    "        test_features_aligned = test_features_encoded.reindex(\n",
    "            columns=train_cols, fill_value=0\n",
    "        )\n",
    "        with open(scaler_path, \"rb\") as f:\n",
    "            scaler = pkl.load(f)\n",
    "        test_features_scaled = scaler.transform(test_features_aligned[numerical_cols])\n",
    "        test_features_final = pd.DataFrame(\n",
    "            test_features_scaled,\n",
    "            index=test_features_aligned.index,\n",
    "            columns=numerical_cols,\n",
    "        ).fillna(0)\n",
    "        test_processed_features = test_features_final[cols_to_keep]\n",
    "    return train_features_final, train_targets, test_processed_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Loop, Train, and Save Predictions\n",
    "\n",
    "This is the main part of the notebook. We will loop through all 16 tasks. For each task, we will:\n",
    "1. Load the training data and the test features.\n",
    "2. Preprocess both.\n",
    "3. Train a model on the training data.\n",
    "4. Generate predictions on the processed test features.\n",
    "5. Save the predictions to a CSV file with the correct name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_for_pair_and_date(\n",
    "    index_event: str, outcome_event: str, model_date: int | None = None, verbose: bool = False\n",
    "):\n",
    "    # normalize model_date for filename\n",
    "    model_date_str = str(model_date) if model_date is not None else \"latest\"\n",
    "    model_filename = f\"xgboost_cox_{index_event}_{outcome_event}_{model_date_str}.ubj\"\n",
    "    model_path = os.path.join(MODEL_CACHE_DIR, model_filename)\n",
    "\n",
    "    # Create model with Cox objective\n",
    "    model = XGBRegressor(\n",
    "        objective=\"survival:cox\",\n",
    "        eval_metric=\"cox-nloglik\",\n",
    "        tree_method=\"hist\",\n",
    "        predictor=\"gpu_predictor\",\n",
    "        device=\"cuda\",\n",
    "        seed=42,\n",
    "        verbosity=0,\n",
    "        max_bin=64,\n",
    "        learning_rate=0.04,\n",
    "        max_depth=5,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.8,\n",
    "        min_child_weight=5,\n",
    "        reg_lambda=1.0,\n",
    "        reg_alpha=0.1,\n",
    "    )\n",
    "\n",
    "    # If model file exists, try to load into the estimator and return the estimator\n",
    "    if os.path.exists(model_path):\n",
    "        if verbose:\n",
    "            print(f\"Loading existing model from {model_path}\")\n",
    "        try:\n",
    "            model.load_model(model_path)\n",
    "            if verbose:\n",
    "                print(f\"model loaded from {model_path}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: failed to load model from {model_path}: {e}. Will retrain.\")\n",
    "\n",
    "    dataset_path = os.path.join(index_event, outcome_event)\n",
    "\n",
    "    # --- Load and Preprocess ---\n",
    "    if verbose:\n",
    "        print(f\"Loading data from {os.path.join(DATA_PATH, dataset_path, 'data.csv')}\")\n",
    "    train_df = pd.read_csv(os.path.join(DATA_PATH, dataset_path, \"data.csv\"))\n",
    "\n",
    "    X_train, y_train, _ = preprocess(train_df, model_date=model_date)\n",
    "\n",
    "    # --- Train Model ---\n",
    "    # Prepare target variables for Cox regression\n",
    "    y_train_duration = y_train[\"timeDiff\"].values\n",
    "    y_train_event = y_train[\"status\"].values\n",
    "\n",
    "    # Fit model: XGBoost Cox expects labels to be the event indicators\n",
    "    # and the sample_weight to be the durations\n",
    "    if verbose:\n",
    "        print(\"Training model...\")\n",
    "    try:\n",
    "        model.fit(X_train, y_train_event, sample_weight=y_train_duration)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Model training failed for {dataset_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Save model: try estimator's save_model, fall back to Booster.save_model\n",
    "    try:\n",
    "        # XGBRegressor implements save_model; call it and confirm file created\n",
    "        model.save_model(model_path)\n",
    "        if verbose:\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            booster = model.get_booster()\n",
    "            booster.save_model(model_path)\n",
    "            if verbose:\n",
    "                print(f\"Model booster saved to {model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to save model to {model_path}: {e}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_for_all_event_pairs(\n",
    "    model_date: int | None = None, verbose: bool = False\n",
    "):\n",
    "    # Define all 16 event pairs\n",
    "    index_events = [\"Liquidated\", \"Borrow\", \"Deposit\", \"Repay\", \"Withdraw\"]\n",
    "    outcome_events = index_events\n",
    "    event_pairs = [\n",
    "        event_pair\n",
    "        for sub_event_pairs in [\n",
    "            [(index_event, outcome_event) for outcome_event in outcome_events]\n",
    "            for index_event in index_events\n",
    "        ]\n",
    "        for event_pair in sub_event_pairs\n",
    "    ]\n",
    "\n",
    "    for index_event, outcome_event in event_pairs:\n",
    "        if index_event == outcome_event and index_event == \"Liquidated\":\n",
    "            continue\n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Training for: {index_event} -> {outcome_event}\")\n",
    "            print(f\"{'='*50}\")\n",
    "\n",
    "        get_model_for_pair_and_date(index_event, outcome_event, model_date=model_date, verbose=verbose)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n\\nAll prediction files have been generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training for: Liquidated -> Borrow\n",
      "==================================================\n",
      "Loading data from ./data/Liquidated/Borrow/data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Model booster saved to ./cache/models/xgboost_cox_Liquidated_Borrow_1751328000.ubj\n",
      "\n",
      "==================================================\n",
      "Training for: Liquidated -> Deposit\n",
      "==================================================\n",
      "Loading data from ./data/Liquidated/Deposit/data.csv\n",
      "Training model...\n",
      "Model booster saved to ./cache/models/xgboost_cox_Liquidated_Deposit_1751328000.ubj\n",
      "\n",
      "==================================================\n",
      "Training for: Liquidated -> Repay\n",
      "==================================================\n",
      "Loading data from ./data/Liquidated/Repay/data.csv\n",
      "Training model...\n",
      "Model booster saved to ./cache/models/xgboost_cox_Liquidated_Repay_1751328000.ubj\n",
      "\n",
      "==================================================\n",
      "Training for: Liquidated -> Withdraw\n",
      "==================================================\n",
      "Loading data from ./data/Liquidated/Withdraw/data.csv\n",
      "Training model...\n",
      "Model booster saved to ./cache/models/xgboost_cox_Liquidated_Withdraw_1751328000.ubj\n",
      "\n",
      "==================================================\n",
      "Training for: Borrow -> Liquidated\n",
      "==================================================\n",
      "Loading data from ./data/Borrow/Liquidated/data.csv\n",
      "Training model...\n",
      "Model booster saved to ./cache/models/xgboost_cox_Borrow_Liquidated_1751328000.ubj\n",
      "\n",
      "==================================================\n",
      "Training for: Borrow -> Borrow\n",
      "==================================================\n",
      "Loading data from ./data/Borrow/Borrow/data.csv\n",
      "Training model...\n",
      "Model booster saved to ./cache/models/xgboost_cox_Borrow_Borrow_1751328000.ubj\n",
      "\n",
      "==================================================\n",
      "Training for: Borrow -> Deposit\n",
      "==================================================\n",
      "Loading data from ./data/Borrow/Deposit/data.csv\n",
      "Training model...\n",
      "Model booster saved to ./cache/models/xgboost_cox_Borrow_Deposit_1751328000.ubj\n",
      "\n",
      "==================================================\n",
      "Training for: Borrow -> Repay\n",
      "==================================================\n",
      "Loading data from ./data/Borrow/Repay/data.csv\n",
      "Training model...\n",
      "Model booster saved to ./cache/models/xgboost_cox_Borrow_Repay_1751328000.ubj\n",
      "\n",
      "==================================================\n",
      "Training for: Borrow -> Withdraw\n",
      "==================================================\n",
      "Loading data from ./data/Borrow/Withdraw/data.csv\n",
      "Training model...\n",
      "Model booster saved to ./cache/models/xgboost_cox_Borrow_Withdraw_1751328000.ubj\n",
      "\n",
      "==================================================\n",
      "Training for: Deposit -> Liquidated\n",
      "==================================================\n",
      "Loading data from ./data/Deposit/Liquidated/data.csv\n",
      "Training model...\n",
      "Model booster saved to ./cache/models/xgboost_cox_Deposit_Liquidated_1751328000.ubj\n",
      "\n",
      "==================================================\n",
      "Training for: Deposit -> Borrow\n",
      "==================================================\n",
      "Loading data from ./data/Deposit/Borrow/data.csv\n",
      "Training model...\n",
      "Model booster saved to ./cache/models/xgboost_cox_Deposit_Borrow_1751328000.ubj\n",
      "\n",
      "==================================================\n",
      "Training for: Deposit -> Deposit\n",
      "==================================================\n",
      "Loading data from ./data/Deposit/Deposit/data.csv\n",
      "Training model...\n",
      "Model booster saved to ./cache/models/xgboost_cox_Deposit_Deposit_1751328000.ubj\n",
      "\n",
      "==================================================\n",
      "Training for: Deposit -> Repay\n",
      "==================================================\n",
      "Loading data from ./data/Deposit/Repay/data.csv\n",
      "Training model...\n",
      "Model booster saved to ./cache/models/xgboost_cox_Deposit_Repay_1751328000.ubj\n",
      "\n",
      "==================================================\n",
      "Training for: Deposit -> Withdraw\n",
      "==================================================\n",
      "Loading data from ./data/Deposit/Withdraw/data.csv\n",
      "Training model...\n",
      "Model booster saved to ./cache/models/xgboost_cox_Deposit_Withdraw_1751328000.ubj\n",
      "\n",
      "==================================================\n",
      "Training for: Repay -> Liquidated\n",
      "==================================================\n",
      "Loading data from ./data/Repay/Liquidated/data.csv\n",
      "Training model...\n",
      "Model booster saved to ./cache/models/xgboost_cox_Repay_Liquidated_1751328000.ubj\n",
      "\n",
      "==================================================\n",
      "Training for: Repay -> Borrow\n",
      "==================================================\n",
      "Loading data from ./data/Repay/Borrow/data.csv\n",
      "Training model...\n",
      "Model booster saved to ./cache/models/xgboost_cox_Repay_Borrow_1751328000.ubj\n",
      "\n",
      "==================================================\n",
      "Training for: Repay -> Deposit\n",
      "==================================================\n",
      "Loading data from ./data/Repay/Deposit/data.csv\n",
      "Training model...\n",
      "Model booster saved to ./cache/models/xgboost_cox_Repay_Deposit_1751328000.ubj\n",
      "\n",
      "==================================================\n",
      "Training for: Repay -> Repay\n",
      "==================================================\n",
      "Loading data from ./data/Repay/Repay/data.csv\n",
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "train_models_for_all_event_pairs(model_date=1751328000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_ranges():\n",
    "    if os.path.exists(os.path.join(CACHE_DIR, \"date_ranges.pkl\")):\n",
    "        with open(os.path.join(CACHE_DIR, \"date_ranges.pkl\"), \"rb\") as f:\n",
    "            return pkl.load(f)\n",
    "    date_df = pd.read_csv(os.path.join(DATA_PATH, \"Withdraw\", \"Withdraw\", \"data.csv\"))\n",
    "    min_date = date_df[\"timestamp\"].min()\n",
    "    max_date = date_df[\"timestamp\"].max()\n",
    "    train_start_date = min_date + 0.4 * (max_date - min_date)\n",
    "    test_start_date = min_date + 0.8 * (max_date - min_date)\n",
    "    train_dates = pd.date_range(start=train_start_date, end=test_start_date, freq=\"2W\")\n",
    "    test_dates = pd.date_range(start=test_start_date, end=max_date, freq=\"2W\")\n",
    "    with open(os.path.join(CACHE_DIR, \"date_ranges.pkl\"), \"wb\") as f:\n",
    "        pkl.dump((train_dates, test_dates), f)\n",
    "    return train_dates, test_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in chain(get_date_ranges()):\n",
    "    train_models_for_all_event_pairs(\n",
    "        model_date=int(date.astype(int) / 10**9), verbose=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "actionAgent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

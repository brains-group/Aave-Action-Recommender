{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FinSurvival Competition: Starter Notebook (AFT Model Prediction Submission)\n",
    "\n",
    "**Objective:** This notebook provides a workflow for creating a valid prediction submission using the `WeibullAFTFitter` model. The competition requires you to submit a `.zip` file containing 16 separate prediction files in CSV format.\n",
    "\n",
    "This notebook will guide you through:\n",
    "1.  Loading the training and test sets for each of the 16 tasks from a single directory.\n",
    "2.  Training a model (using `WeibullAFTFitter` as an example).\n",
    "3.  Generating predictions on the test set in the required format.\n",
    "4.  Saving each set of predictions to a correctly named CSV file.\n",
    "5.  Zipping all 16 prediction files for submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# pip install -q pandas lifelines==0.27.8 scikit-learn==1.2.2 scikit-survival==0.21.0 numpy\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from lifelines import WeibullAFTFitter\n",
    "from lifelines.exceptions import ConvergenceError\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Tuple, Optional\n",
    "from utils.constants import *\n",
    "import pickle as pkl\n",
    "from xgboost import XGBRegressor\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "\n",
    "# Module-level cache for loaded/trained models to reuse across calls\n",
    "MODELS_CACHE: dict = {}\n",
    "\n",
    "# Module-level cache for preprocessing artifacts (scaler, columns, categories)\n",
    "PREPROCESS_CACHE: dict = {}\n",
    "\n",
    "def get_concordance_index(\n",
    "    test_df: pd.DataFrame, \n",
    "    predictions: np.ndarray\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculates the concordance index for survival models using scikit-survival.\n",
    "    Replaces any NaN predictions with -1.\n",
    "    \"\"\"\n",
    "    # Replace NaN predictions with a value representing the worst possible score (shortest survival)\n",
    "    # Using -1 is a robust way to handle failed predictions without causing numerical errors.\n",
    "    predictions[np.isnan(predictions)] = -1\n",
    "    \n",
    "    event_indicator = test_df['status'].astype(bool)\n",
    "    event_time = test_df['timeDiff']\n",
    "\n",
    "    # Handle cases where all events are censored or all are non-censored in the test set\n",
    "    if len(np.unique(event_indicator)) == 1:\n",
    "        return 0.5  # Return a neutral score\n",
    "\n",
    "    c_index, _, _, _, _ = concordance_index_censored(\n",
    "        event_indicator, event_time, -predictions\n",
    "    )\n",
    "    \n",
    "    return c_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define a Preprocessing Function\n",
    "\n",
    "Even though you are not submitting this code, you will still need a preprocessing pipeline to train your models effectively. You can use the one below as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    train_df_with_labels: Optional[pd.DataFrame] = None,\n",
    "    test_features_df: Optional[pd.DataFrame] = None,\n",
    "    model_date: Optional[int] = None,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "\n",
    "    # Create unique prefix for saving/loading preprocessing objects\n",
    "    unique_prefix = (\n",
    "        (\n",
    "            present_dataframe := (\n",
    "                train_df_with_labels\n",
    "                if train_df_with_labels is not None\n",
    "                else test_features_df\n",
    "            )\n",
    "        )[\"Index Event\"].iloc[0]\n",
    "        + \"_\"\n",
    "        + present_dataframe[\"Outcome Event\"].iloc[0]\n",
    "        + (f\"_{model_date}_\" if model_date is not None else \"_\")\n",
    "    )\n",
    "    # Define paths for saving/loading preprocessing objects\n",
    "    scaler_path = os.path.join(DATA_CACHE_DIR, unique_prefix + \"scaler.pkl\")\n",
    "    train_cols = os.path.join(DATA_CACHE_DIR, unique_prefix + \"train_cols.pkl\")\n",
    "    top_categories_dict_path = os.path.join(\n",
    "        DATA_CACHE_DIR, unique_prefix + \"top_categories_dict.pkl\"\n",
    "    )\n",
    "    categorical_cols_path = os.path.join(\n",
    "        DATA_CACHE_DIR, unique_prefix + \"categorical_cols.pkl\"\n",
    "    )\n",
    "    numerical_cols_path = os.path.join(\n",
    "        DATA_CACHE_DIR, unique_prefix + \"numerical_cols.pkl\"\n",
    "    )\n",
    "    cols_to_keep_path = os.path.join(DATA_CACHE_DIR, unique_prefix + \"cols_to_keep.pkl\")\n",
    "\n",
    "    target_columns = [\"timeDiff\", \"status\"]\n",
    "    cols_to_drop = [\n",
    "        \"id\",\n",
    "        \"user\",\n",
    "        \"pool\",\n",
    "        \"Index Event\",\n",
    "        \"Outcome Event\",\n",
    "        \"type\",\n",
    "        \"timestamp\",\n",
    "    ]\n",
    "\n",
    "    if train_df_with_labels is not None:\n",
    "        if model_date is not None:\n",
    "            # model_date may be a pandas.Timestamp while dataframe timestamps are numeric.\n",
    "            # Convert model_date to numeric epoch seconds for a safe comparison.\n",
    "            if isinstance(model_date, pd.Timestamp):\n",
    "                model_date_value = model_date.timestamp()\n",
    "            else:\n",
    "                try:\n",
    "                    model_date_value = float(model_date)\n",
    "                except Exception:\n",
    "                    model_date_value = model_date\n",
    "\n",
    "            train_df_with_labels = train_df_with_labels[\n",
    "                (train_df_with_labels[\"timestamp\"] + train_df_with_labels[\"timeDiff\"])\n",
    "                <= model_date_value\n",
    "            ]\n",
    "\n",
    "        # Separate features and targets (and drop unneeded columns from features)\n",
    "        train_targets = train_df_with_labels[target_columns]\n",
    "        train_features = train_df_with_labels.drop(\n",
    "            columns=target_columns + cols_to_drop, errors=\"ignore\"\n",
    "        )\n",
    "\n",
    "        # Make uncommon categories \"Other\" and one-hot encode categorical features\n",
    "        categorical_cols = train_features.select_dtypes(\n",
    "            include=[\"object\", \"category\"]\n",
    "        ).columns\n",
    "        top_categories_dict = {}\n",
    "        for col in categorical_cols:\n",
    "            top_categories_dict[col] = (\n",
    "                train_features[col].value_counts().nlargest(10).index\n",
    "            )\n",
    "            train_features[col] = train_features[col].where(\n",
    "                train_features[col].isin(top_categories_dict[col]), \"Other\"\n",
    "            )\n",
    "        train_features_encoded = pd.get_dummies(\n",
    "            train_features, columns=categorical_cols, dummy_na=True, drop_first=True\n",
    "        )\n",
    "\n",
    "        # Standardize numerical features\n",
    "        numerical_cols = train_features_encoded.select_dtypes(include=np.number).columns\n",
    "        scaler = StandardScaler()\n",
    "        train_features_scaled = scaler.fit_transform(\n",
    "            train_features_encoded[numerical_cols]\n",
    "        )\n",
    "        train_features_final = pd.DataFrame(\n",
    "            train_features_scaled,\n",
    "            index=train_features_encoded.index,\n",
    "            columns=numerical_cols,\n",
    "        ).fillna(0)\n",
    "\n",
    "        # Remove zero-variance columns\n",
    "        cols_to_keep = train_features_final.columns[train_features_final.var() != 0]\n",
    "        train_features_final = train_features_final[cols_to_keep]\n",
    "\n",
    "        # Save preprocessing objects\n",
    "        with open(scaler_path, \"wb\") as f:\n",
    "            pkl.dump(scaler, f)\n",
    "        with open(train_cols, \"wb\") as f:\n",
    "            pkl.dump(train_features_encoded.columns, f)\n",
    "        with open(top_categories_dict_path, \"wb\") as f:\n",
    "            pkl.dump(top_categories_dict, f)\n",
    "        with open(categorical_cols_path, \"wb\") as f:\n",
    "            pkl.dump(categorical_cols, f)\n",
    "        with open(numerical_cols_path, \"wb\") as f:\n",
    "            pkl.dump(numerical_cols, f)\n",
    "        with open(cols_to_keep_path, \"wb\") as f:\n",
    "            pkl.dump(cols_to_keep, f)\n",
    "\n",
    "        # Populate in-memory preprocess cache for fast reuse\n",
    "        PREPROCESS_CACHE[unique_prefix] = {\n",
    "            \"scaler\": scaler,\n",
    "            \"train_cols\": train_features_encoded.columns,\n",
    "            \"top_categories_dict\": top_categories_dict,\n",
    "            \"categorical_cols\": categorical_cols,\n",
    "            \"numerical_cols\": numerical_cols,\n",
    "            \"cols_to_keep\": cols_to_keep,\n",
    "        }\n",
    "    else:\n",
    "        train_features_final = None\n",
    "        train_targets = None\n",
    "\n",
    "    # Process test features if provided\n",
    "    if test_features_df is not None:\n",
    "        if unique_prefix not in PREPROCESS_CACHE:\n",
    "            PREPROCESS_CACHE[unique_prefix] = {}\n",
    "            with open(categorical_cols_path, \"rb\") as f:\n",
    "                PREPROCESS_CACHE[unique_prefix][\"categorical_cols\"] = pkl.load(f)\n",
    "            with open(top_categories_dict_path, \"rb\") as f:\n",
    "                PREPROCESS_CACHE[unique_prefix][\"top_categories_dict\"] = pkl.load(f)\n",
    "            with open(numerical_cols_path, \"rb\") as f:\n",
    "                PREPROCESS_CACHE[unique_prefix][\"numerical_cols\"] = pkl.load(f)\n",
    "            with open(cols_to_keep_path, \"rb\") as f:\n",
    "                PREPROCESS_CACHE[unique_prefix][\"cols_to_keep\"] = pkl.load(f)\n",
    "            with open(train_cols, \"rb\") as f:\n",
    "                PREPROCESS_CACHE[unique_prefix][\"train_cols\"] = pkl.load(f)\n",
    "            with open(scaler_path, \"rb\") as f:\n",
    "                PREPROCESS_CACHE[unique_prefix][\"scaler\"] = pkl.load(f)\n",
    "\n",
    "        test_features = test_features_df.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "        categorical_cols = PREPROCESS_CACHE[unique_prefix][\"categorical_cols\"]\n",
    "        top_categories_dict = PREPROCESS_CACHE[unique_prefix][\"top_categories_dict\"]\n",
    "        for col in categorical_cols:\n",
    "            top_categories = top_categories_dict[col]\n",
    "            test_features[col] = test_features[col].where(\n",
    "                test_features[col].isin(top_categories), \"Other\"\n",
    "            )\n",
    "        test_features_encoded = pd.get_dummies(\n",
    "            test_features, columns=categorical_cols, dummy_na=True, drop_first=True\n",
    "        )\n",
    "        train_cols = PREPROCESS_CACHE[unique_prefix][\"train_cols\"]\n",
    "        test_features_aligned = test_features_encoded.reindex(\n",
    "            columns=train_cols, fill_value=0\n",
    "        )\n",
    "        scaler = PREPROCESS_CACHE[unique_prefix][\"scaler\"]\n",
    "        numerical_cols = PREPROCESS_CACHE[unique_prefix][\"numerical_cols\"]\n",
    "        test_features_scaled = scaler.transform(test_features_aligned[numerical_cols])\n",
    "        test_features_final = pd.DataFrame(\n",
    "            test_features_scaled,\n",
    "            index=test_features_aligned.index,\n",
    "            columns=numerical_cols,\n",
    "        ).fillna(0)\n",
    "        cols_to_keep = PREPROCESS_CACHE[unique_prefix][\"cols_to_keep\"]\n",
    "        # logger.debug(\"cols_to_keep:%s\", cols_to_keep)\n",
    "        test_processed_features = test_features_final[cols_to_keep]\n",
    "    else:\n",
    "        test_processed_features = None\n",
    "    return train_features_final, train_targets, test_processed_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Loop, Train, and Save Predictions\n",
    "\n",
    "This is the main part of the notebook. We will loop through all 16 tasks. For each task, we will:\n",
    "1. Load the training data and the test features.\n",
    "2. Preprocess both.\n",
    "3. Train a model on the training data.\n",
    "4. Generate predictions on the processed test features.\n",
    "5. Save the predictions to a CSV file with the correct name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Borrow -> Borrow\n",
      "==================================================\n",
      "Generating predictions for Borrow/Borrow...\n",
      "Concordance index: 0.5366361980962028\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Borrow -> Deposit\n",
      "==================================================\n",
      "Generating predictions for Borrow/Deposit...\n",
      "Concordance index: 0.46057613530236197\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Borrow -> Repay\n",
      "==================================================\n",
      "Generating predictions for Borrow/Repay...\n",
      "Concordance index: 0.48514464943305036\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Borrow -> Withdraw\n",
      "==================================================\n",
      "Generating predictions for Borrow/Withdraw...\n",
      "Concordance index: 0.46939812055149294\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Borrow -> Liquidated\n",
      "==================================================\n",
      "Generating predictions for Borrow/Liquidated...\n",
      "Concordance index: 0.5239693084090911\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Deposit -> Borrow\n",
      "==================================================\n",
      "Generating predictions for Deposit/Borrow...\n",
      "Concordance index: 0.5207504196273232\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Deposit -> Deposit\n",
      "==================================================\n",
      "Generating predictions for Deposit/Deposit...\n",
      "Concordance index: 0.5221460398331528\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Deposit -> Repay\n",
      "==================================================\n",
      "Generating predictions for Deposit/Repay...\n",
      "Concordance index: 0.5165944188826016\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Deposit -> Withdraw\n",
      "==================================================\n",
      "Generating predictions for Deposit/Withdraw...\n",
      "Concordance index: 0.49698047165471265\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Deposit -> Liquidated\n",
      "==================================================\n",
      "Generating predictions for Deposit/Liquidated...\n",
      "Concordance index: 0.523569316712631\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Repay -> Borrow\n",
      "==================================================\n",
      "Generating predictions for Repay/Borrow...\n",
      "Concordance index: 0.5623494221725633\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Repay -> Deposit\n",
      "==================================================\n",
      "Generating predictions for Repay/Deposit...\n",
      "Concordance index: 0.4277044485185033\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Repay -> Repay\n",
      "==================================================\n",
      "Generating predictions for Repay/Repay...\n",
      "Concordance index: 0.5622503070352128\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Repay -> Withdraw\n",
      "==================================================\n",
      "Generating predictions for Repay/Withdraw...\n",
      "Concordance index: 0.47494345967868007\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Repay -> Liquidated\n",
      "==================================================\n",
      "Generating predictions for Repay/Liquidated...\n",
      "Concordance index: 0.5147949459316832\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Withdraw -> Borrow\n",
      "==================================================\n",
      "Generating predictions for Withdraw/Borrow...\n",
      "Concordance index: 0.5097342022786453\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Withdraw -> Deposit\n",
      "==================================================\n",
      "Generating predictions for Withdraw/Deposit...\n",
      "Concordance index: 0.5050247427769422\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Withdraw -> Repay\n",
      "==================================================\n",
      "Generating predictions for Withdraw/Repay...\n",
      "Concordance index: 0.49697036973097797\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Withdraw -> Withdraw\n",
      "==================================================\n",
      "Generating predictions for Withdraw/Withdraw...\n",
      "Concordance index: 0.5\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Withdraw -> Liquidated\n",
      "==================================================\n",
      "Generating predictions for Withdraw/Liquidated...\n",
      "Concordance index: 0.5110298733602504\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Liquidated -> Borrow\n",
      "==================================================\n",
      "Generating predictions for Liquidated/Borrow...\n",
      "Concordance index: 0.5698081854924686\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Liquidated -> Deposit\n",
      "==================================================\n",
      "Generating predictions for Liquidated/Deposit...\n",
      "Concordance index: 0.5768413082624295\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Liquidated -> Repay\n",
      "==================================================\n",
      "Generating predictions for Liquidated/Repay...\n",
      "Concordance index: 0.5944789397721415\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Liquidated -> Withdraw\n",
      "==================================================\n",
      "Generating predictions for Liquidated/Withdraw...\n",
      "Concordance index: 0.5\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Liquidated -> Liquidated\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m test_features_df = test_df[feature_cols]\n\u001b[32m     37\u001b[39m test_references_df = test_df[reference_cols]\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m X_train, y_train, X_test_processed = \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_features_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# --- Train Model ---\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mpreprocess\u001b[39m\u001b[34m(train_df_with_labels, test_features_df, model_date)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreprocess\u001b[39m(\n\u001b[32m      2\u001b[39m     train_df_with_labels: Optional[pd.DataFrame] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m      3\u001b[39m     test_features_df: Optional[pd.DataFrame] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Create unique prefix for saving/loading preprocessing objects\u001b[39;00m\n\u001b[32m      8\u001b[39m     unique_prefix = (\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m         \u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpresent_dataframe\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtrain_df_with_labels\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_df_with_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_features_df\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mIndex Event\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     16\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     17\u001b[39m         + present_dataframe[\u001b[33m\"\u001b[39m\u001b[33mOutcome Event\u001b[39m\u001b[33m\"\u001b[39m].iloc[\u001b[32m0\u001b[39m]\n\u001b[32m     18\u001b[39m         + (\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_date \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m     )\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# Define paths for saving/loading preprocessing objects\u001b[39;00m\n\u001b[32m     21\u001b[39m     scaler_path = os.path.join(DATA_CACHE_DIR, unique_prefix + \u001b[33m\"\u001b[39m\u001b[33mscaler.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/finsur2/lib/python3.12/site-packages/pandas/core/indexing.py:1191\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1189\u001b[39m maybe_callable = com.apply_if_callable(key, \u001b[38;5;28mself\u001b[39m.obj)\n\u001b[32m   1190\u001b[39m maybe_callable = \u001b[38;5;28mself\u001b[39m._check_deprecated_callable_usage(key, maybe_callable)\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/finsur2/lib/python3.12/site-packages/pandas/core/indexing.py:1752\u001b[39m, in \u001b[36m_iLocIndexer._getitem_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot index by location index with a non-integer key\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1751\u001b[39m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1752\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1754\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._ixs(key, axis=axis)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/finsur2/lib/python3.12/site-packages/pandas/core/indexing.py:1685\u001b[39m, in \u001b[36m_iLocIndexer._validate_integer\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1683\u001b[39m len_axis = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.obj._get_axis(axis))\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key >= len_axis \u001b[38;5;129;01mor\u001b[39;00m key < -len_axis:\n\u001b[32m-> \u001b[39m\u001b[32m1685\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33msingle positional indexer is out-of-bounds\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "# Define path to the single participant data folder.\n",
    "PARTICIPANT_DATA_PATH = \"./data/\"\n",
    "\n",
    "# Define all 16 event pairs\n",
    "index_events = [\"Borrow\", \"Deposit\", \"Repay\", \"Withdraw\", \"Liquidated\"]\n",
    "outcome_events = index_events\n",
    "event_pairs = [\n",
    "    (index_event, outcome_event)\n",
    "    for index_event in index_events\n",
    "    for outcome_event in outcome_events\n",
    "]\n",
    "\n",
    "for index_event, outcome_event in event_pairs:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing and Predicting for: {index_event} -> {outcome_event}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    dataset_path = os.path.join(index_event, outcome_event)\n",
    "\n",
    "    # --- Load and Preprocess ---\n",
    "    try:\n",
    "        data_df = pd.read_csv(\n",
    "            os.path.join(PARTICIPANT_DATA_PATH, dataset_path, \"data.csv\")\n",
    "        )\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Data not found for {dataset_path}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    if not data_df:\n",
    "        continue\n",
    "\n",
    "    buffer_duration = 30 * 60 * 60 * 24\n",
    "    train_cutoff = 1722526142 - buffer_duration\n",
    "\n",
    "    train_df = data_df[data_df[\"timestamp\"] <= train_cutoff]\n",
    "    test_df = data_df[data_df[\"timestamp\"] > train_cutoff]\n",
    "    reference_cols = [\"timeDiff\", \"status\"]\n",
    "    feature_cols = [col for col in train_df.columns if col not in reference_cols]\n",
    "    test_features_df = test_df[feature_cols]\n",
    "    test_references_df = test_df[reference_cols]\n",
    "\n",
    "    X_train, y_train, X_test_processed = preprocess(train_df, test_features_df)\n",
    "\n",
    "    # --- Train Model ---\n",
    "    try:\n",
    "        lifelines_train_df = pd.concat(\n",
    "            [X_train, y_train.reset_index(drop=True)], axis=1\n",
    "        )\n",
    "        lifelines_train_df = lifelines_train_df.loc[\n",
    "            lifelines_train_df[\"timeDiff\"] > 0\n",
    "        ].copy()\n",
    "\n",
    "        model = XGBRegressor(\n",
    "            objective=\"survival:cox\",\n",
    "            eval_metric=\"cox-nloglik\",\n",
    "            tree_method=\"hist\",\n",
    "            predictor=\"gpu_predictor\",\n",
    "            device=\"cuda\",\n",
    "            seed=42,\n",
    "            verbosity=0,\n",
    "            max_bin=64,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=6,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            min_child_weight=5,\n",
    "            reg_lambda=1.0,\n",
    "            reg_alpha=0.1,\n",
    "        )\n",
    "        y_train_duration = y_train[\"timeDiff\"].values\n",
    "        y_train_event = y_train[\"status\"].values\n",
    "        model.fit(X_train, y_train_event, sample_weight=y_train_duration)\n",
    "\n",
    "        # --- Generate and Save Predictions ---\n",
    "        print(f\"Generating predictions for {dataset_path}...\")\n",
    "        # Use the processed test features to make predictions\n",
    "        predictions = model.predict(X_test_processed, output_margin=True)\n",
    "\n",
    "        print(f\"Concordance index: {get_concordance_index(test_references_df, predictions)}\")\n",
    "\n",
    "    except (ConvergenceError, ValueError) as e:\n",
    "        print(\n",
    "            f\"\\nERROR: The model for {dataset_path} failed to train. No prediction file will be created.\"\n",
    "        )\n",
    "        print(f\"Details: {e}\")\n",
    "\n",
    "print(\"\\n\\nAll prediction files have been generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finsur2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

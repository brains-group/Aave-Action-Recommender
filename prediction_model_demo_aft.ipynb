{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "# Install required packages\n",
    "# pip install -q pandas xgboost scikit-learn numpy pyreadr\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Tuple, Optional\n",
    "import pickle as pkl\n",
    "from itertools import chain\n",
    "import pyreadr\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data/\"\n",
    "CACHE_DIR = \"./cache/\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "MODEL_CACHE_DIR = os.path.join(CACHE_DIR, \"models\")\n",
    "os.makedirs(MODEL_CACHE_DIR, exist_ok=True)\n",
    "DATA_CACHE_DIR = os.path.join(CACHE_DIR, \"data\")\n",
    "os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
    "RESULTS_CACHE_DIR = os.path.join(CACHE_DIR, \"results\")\n",
    "os.makedirs(RESULTS_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "EVENTS = [\"Deposit\", \"Withdraw\", \"Repay\", \"Borrow\", \"Liquidated\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    train_df_with_labels: Optional[pd.DataFrame] = None,\n",
    "    test_features_df: Optional[pd.DataFrame] = None,\n",
    "    model_date: Optional[int] = None,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "\n",
    "    # Create unique prefix for saving/loading preprocessing objects\n",
    "    unique_prefix = (\n",
    "        (\n",
    "            present_dataframe := (\n",
    "                train_df_with_labels\n",
    "                if train_df_with_labels is not None\n",
    "                else test_features_df\n",
    "            )\n",
    "        )[\"Index Event\"].iloc[0]\n",
    "        + \"_\"\n",
    "        + present_dataframe[\"Outcome Event\"].iloc[0]\n",
    "        + (f\"_{model_date}_\" if model_date is not None else \"_\")\n",
    "    )\n",
    "    # Define paths for saving/loading preprocessing objects\n",
    "    scaler_path = os.path.join(DATA_CACHE_DIR, unique_prefix + \"scaler.pkl\")\n",
    "    train_cols = os.path.join(DATA_CACHE_DIR, unique_prefix + \"train_cols.pkl\")\n",
    "    top_categories_dict_path = os.path.join(\n",
    "        DATA_CACHE_DIR, unique_prefix + \"top_categories_dict.pkl\"\n",
    "    )\n",
    "\n",
    "    if train_df_with_labels is not None:\n",
    "        if model_date is not None:\n",
    "            train_df_with_labels = train_df_with_labels[\n",
    "                (train_df_with_labels[\"timestamp\"] + train_df_with_labels[\"timeDiff\"])\n",
    "                <= model_date\n",
    "            ]\n",
    "\n",
    "        # Separate features and targets (and drop unneeded columns from features)\n",
    "        target_columns = [\"timeDiff\", \"status\"]\n",
    "        train_targets = train_df_with_labels[target_columns]\n",
    "        cols_to_drop = [\n",
    "            \"id\",\n",
    "            \"user\",\n",
    "            \"pool\",\n",
    "            \"Index Event\",\n",
    "            \"Outcome Event\",\n",
    "            \"type\",\n",
    "            \"timestamp\",\n",
    "        ]\n",
    "        train_features = train_df_with_labels.drop(\n",
    "            columns=target_columns + cols_to_drop, errors=\"ignore\"\n",
    "        )\n",
    "\n",
    "        # Make uncommon categories \"Other\" and one-hot encode categorical features\n",
    "        categorical_cols = train_features.select_dtypes(\n",
    "            include=[\"object\", \"category\"]\n",
    "        ).columns\n",
    "        top_categories_dict = {}\n",
    "        for col in categorical_cols:\n",
    "            if col not in top_categories_dict:\n",
    "                top_categories_dict[col] = (\n",
    "                    train_features[col].value_counts().nlargest(10).index\n",
    "                )\n",
    "            train_features[col] = train_features[col].where(\n",
    "                train_features[col].isin(top_categories_dict[col]), \"Other\"\n",
    "            )\n",
    "        train_features_encoded = pd.get_dummies(\n",
    "            train_features, columns=categorical_cols, dummy_na=True, drop_first=True\n",
    "        )\n",
    "\n",
    "        # Standardize numerical features\n",
    "        numerical_cols = train_features_encoded.select_dtypes(include=np.number).columns\n",
    "        scaler = StandardScaler()\n",
    "        train_features_scaled = scaler.fit_transform(\n",
    "            train_features_encoded[numerical_cols]\n",
    "        )\n",
    "        train_features_final = pd.DataFrame(\n",
    "            train_features_scaled,\n",
    "            index=train_features_encoded.index,\n",
    "            columns=numerical_cols,\n",
    "        ).fillna(0)\n",
    "\n",
    "        # Remove zero-variance columns\n",
    "        cols_to_keep = train_features_final.columns[train_features_final.var() != 0]\n",
    "        train_features_final = train_features_final[cols_to_keep]\n",
    "\n",
    "        # Save preprocessing objects\n",
    "        with open(scaler_path, \"wb\") as f:\n",
    "            pkl.dump(scaler, f)\n",
    "        with open(train_cols, \"wb\") as f:\n",
    "            pkl.dump(train_features_encoded.columns, f)\n",
    "        with open(top_categories_dict_path, \"wb\") as f:\n",
    "            pkl.dump(top_categories_dict, f)\n",
    "\n",
    "    # Process test features if provided\n",
    "    test_processed_features = None\n",
    "    if test_features_df is not None:\n",
    "        test_features = test_features_df.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "        with open(top_categories_dict_path, \"rb\") as f:\n",
    "            top_categories_dict = pkl.load(f)\n",
    "        for col in categorical_cols:\n",
    "            top_categories = top_categories_dict[col]\n",
    "            test_features[col] = test_features[col].where(\n",
    "                test_features[col].isin(top_categories), \"Other\"\n",
    "            )\n",
    "        test_features_encoded = pd.get_dummies(\n",
    "            test_features, columns=categorical_cols, dummy_na=True, drop_first=True\n",
    "        )\n",
    "        with open(train_cols, \"rb\") as f:\n",
    "            train_cols = pkl.load(f)\n",
    "        test_features_aligned = test_features_encoded.reindex(\n",
    "            columns=train_cols, fill_value=0\n",
    "        )\n",
    "        with open(scaler_path, \"rb\") as f:\n",
    "            scaler = pkl.load(f)\n",
    "        test_features_scaled = scaler.transform(test_features_aligned[numerical_cols])\n",
    "        test_features_final = pd.DataFrame(\n",
    "            test_features_scaled,\n",
    "            index=test_features_aligned.index,\n",
    "            columns=numerical_cols,\n",
    "        ).fillna(0)\n",
    "        test_processed_features = test_features_final[cols_to_keep]\n",
    "    return train_features_final, train_targets, test_processed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_for_pair_and_date(\n",
    "    index_event: str,\n",
    "    outcome_event: str,\n",
    "    model_date: int | None = None,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    # normalize model_date for filename\n",
    "    model_date_str = str(model_date) if model_date is not None else \"latest\"\n",
    "    model_filename = f\"xgboost_cox_{index_event}_{outcome_event}_{model_date_str}.ubj\"\n",
    "    model_path = os.path.join(MODEL_CACHE_DIR, model_filename)\n",
    "\n",
    "    # Create model with Cox objective\n",
    "    model = XGBRegressor(\n",
    "        objective=\"survival:cox\",\n",
    "        eval_metric=\"cox-nloglik\",\n",
    "        tree_method=\"hist\",\n",
    "        predictor=\"gpu_predictor\",\n",
    "        device=\"cuda\",\n",
    "        seed=42,\n",
    "        verbosity=0,\n",
    "        max_bin=64,\n",
    "        learning_rate=0.04,\n",
    "        max_depth=5,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.8,\n",
    "        min_child_weight=5,\n",
    "        reg_lambda=1.0,\n",
    "        reg_alpha=0.1,\n",
    "    )\n",
    "\n",
    "    # If model file exists, try to load into the estimator and return the estimator\n",
    "    if os.path.exists(model_path):\n",
    "        if verbose:\n",
    "            print(f\"Loading existing model from {model_path}\")\n",
    "        try:\n",
    "            model.load_model(model_path)\n",
    "            if verbose:\n",
    "                print(f\"model loaded from {model_path}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Warning: failed to load model from {model_path}: {e}. Will retrain.\"\n",
    "            )\n",
    "\n",
    "    dataset_path = os.path.join(index_event, outcome_event)\n",
    "\n",
    "    # --- Load and Preprocess ---\n",
    "    if verbose:\n",
    "        print(f\"Loading data from {os.path.join(DATA_PATH, dataset_path, 'data.csv')}\")\n",
    "    train_df = pd.read_csv(os.path.join(DATA_PATH, dataset_path, \"data.csv\"))\n",
    "\n",
    "    X_train, y_train, _ = preprocess(train_df, model_date=model_date)\n",
    "\n",
    "    # --- Train Model ---\n",
    "    # Prepare target variables for Cox regression\n",
    "    y_train_duration = y_train[\"timeDiff\"].values\n",
    "    y_train_event = y_train[\"status\"].values\n",
    "\n",
    "    # Fit model: XGBoost Cox expects labels to be the event indicators\n",
    "    # and the sample_weight to be the durations\n",
    "    if verbose:\n",
    "        print(\"Training model...\")\n",
    "    try:\n",
    "        model.fit(X_train, y_train_event, sample_weight=y_train_duration)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Model training failed for {dataset_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Save model: try estimator's save_model, fall back to Booster.save_model\n",
    "    try:\n",
    "        # XGBRegressor implements save_model; call it and confirm file created\n",
    "        model.save_model(model_path)\n",
    "        if verbose:\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            booster = model.get_booster()\n",
    "            booster.save_model(model_path)\n",
    "            if verbose:\n",
    "                print(f\"Model booster saved to {model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to save model to {model_path}: {e}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_for_all_event_pairs(\n",
    "    model_date: int | None = None, verbose: bool = False\n",
    "):\n",
    "    # Define all 16 event pairs\n",
    "    index_events = EVENTS\n",
    "    outcome_events = index_events\n",
    "    event_pairs = [\n",
    "        event_pair\n",
    "        for sub_event_pairs in [\n",
    "            [(index_event, outcome_event) for outcome_event in outcome_events]\n",
    "            for index_event in index_events\n",
    "        ]\n",
    "        for event_pair in sub_event_pairs\n",
    "    ]\n",
    "\n",
    "    for index_event, outcome_event in event_pairs:\n",
    "        if index_event == outcome_event and index_event == \"Liquidated\":\n",
    "            continue\n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Training for: {index_event} -> {outcome_event}\")\n",
    "            print(f\"{'='*50}\")\n",
    "\n",
    "        get_model_for_pair_and_date(\n",
    "            index_event, outcome_event, model_date=model_date, verbose=verbose\n",
    "        )\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n\\nAll prediction files have been generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_ranges():\n",
    "    if os.path.exists(os.path.join(CACHE_DIR, \"date_ranges.pkl\")):\n",
    "        with open(os.path.join(CACHE_DIR, \"date_ranges.pkl\"), \"rb\") as f:\n",
    "            return pkl.load(f)\n",
    "    transactions_df = pyreadr.read_r(\"./data/transactions.rds\")[None]\n",
    "    min_date = transactions_df[\"timestamp\"].min() * 1e9\n",
    "    print(min_date)\n",
    "    max_date = transactions_df[\"timestamp\"].max() * 1e9\n",
    "    print(max_date)\n",
    "    train_start_date = min_date + 0.4 * (max_date - min_date)\n",
    "    print(train_start_date)\n",
    "    test_start_date = min_date + 0.8 * (max_date - min_date)\n",
    "    print(test_start_date)\n",
    "    train_dates = pd.date_range(start=train_start_date, end=test_start_date, freq=\"2W\")\n",
    "    test_dates = pd.date_range(start=test_start_date, end=max_date, freq=\"2W\")\n",
    "    with open(os.path.join(CACHE_DIR, \"date_ranges.pkl\"), \"wb\") as f:\n",
    "        pkl.dump((train_dates, test_dates), f)\n",
    "    return train_dates, test_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for date in chain(*get_date_ranges()):\n",
    "#     train_models_for_all_event_pairs(model_date=date.timestamp(), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_history(user_id: str, up_to_timestamp: int) -> pd.DataFrame:\n",
    "    all_events = []\n",
    "    for index_event in EVENTS:\n",
    "        for outcome_event in EVENTS:\n",
    "            event_path = os.path.join(DATA_PATH, index_event, outcome_event, \"data.csv\")\n",
    "            if os.path.exists(event_path):\n",
    "                event_df = pd.read_csv(event_path)\n",
    "                user_events = event_df[\n",
    "                    (event_df[\"user\"] == user_id)\n",
    "                    & (event_df[\"timestamp\"] <= up_to_timestamp)\n",
    "                ]\n",
    "                all_events.append(user_events)\n",
    "    if all_events:\n",
    "        user_history_df = pd.concat(all_events).sort_values(by=\"timestamp\")\n",
    "    else:\n",
    "        user_history_df = pd.DataFrame()\n",
    "    return user_history_df\n",
    "\n",
    "\n",
    "def get_transaction_history_predictions(row: pd.Series) -> pd.DataFrame:\n",
    "    results_cache_file = (\n",
    "        RESULTS_CACHE_DIR + f\"{row['user']}_{row['timestamp']}_{row['amount']}.pkl\"\n",
    "    )\n",
    "    if os.path.exists(results_cache_file):\n",
    "        with open(results_cache_file, \"rb\") as f:\n",
    "            return pkl.load(f)\n",
    "\n",
    "    results = {}\n",
    "    train_dates, test_dates = get_date_ranges()\n",
    "    dates = train_dates.union(test_dates)\n",
    "\n",
    "    user_history = get_user_history(\n",
    "        user_id=row[\"user\"], up_to_timestamp=row[\"timestamp\"]\n",
    "    )\n",
    "\n",
    "    model_date = dates[dates <= pd.to_datetime(row[\"timestamp\"], unit=\"s\")].max()\n",
    "    for _, history_row in user_history.iterrows():\n",
    "        history_timestamp = history_row[\"timestamp\"]\n",
    "        results[history_timestamp] = {}\n",
    "        history_row = history_row.copy()\n",
    "\n",
    "        index_event = history_row[\"Index Event\"].title()\n",
    "        for outcome_event in [\n",
    "            \"Liquidated\",\n",
    "            \"Borrow\",\n",
    "            \"Deposit\",\n",
    "            \"Repay\",\n",
    "            \"Withdraw\",\n",
    "        ]:\n",
    "            model = get_model_for_pair_and_date(\n",
    "                index_event, outcome_event, model_date=model_date, verbose=True\n",
    "            )\n",
    "\n",
    "            if model is None:\n",
    "                results[history_timestamp][outcome_event] = None\n",
    "                continue\n",
    "\n",
    "            history_row[\"Outcome Event\"] = outcome_event\n",
    "            _, _, test_features = preprocess(\n",
    "                test_features_df=history_row.to_frame().T,\n",
    "                model_date=model_date,\n",
    "            )\n",
    "\n",
    "            prediction = model.predict(test_features)\n",
    "            results[history_timestamp][outcome_event] = prediction[0]\n",
    "\n",
    "    with open(\n",
    "        results_cache_file,\n",
    "        \"wb\",\n",
    "    ) as f:\n",
    "        pkl.dump(results, f)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trend_slope(data):\n",
    "    \"\"\"\n",
    "    Calculates the linear regression slope of a dataset to determine the trend.\n",
    "\n",
    "    Args:\n",
    "        data (dict): A dictionary where keys are timestamps (int/float)\n",
    "                     and values are numbers.\n",
    "\n",
    "    Returns:\n",
    "        float: The slope of the trend line.\n",
    "               > 0 means increasing, < 0 means decreasing.\n",
    "               Returns 0.0 if not enough data.\n",
    "    \"\"\"\n",
    "    if len(data) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    # 1. Sort data by timestamp (keys)\n",
    "    sorted_items = sorted(data.items())\n",
    "\n",
    "    # 2. Extract x (timestamps) and y (values)\n",
    "    # We subtract the first timestamp from all x values to normalize them\n",
    "    # (starts at time 0). This prevents precision errors with large timestamps.\n",
    "    start_time = sorted_items[0][0]\n",
    "    xs = [x - start_time for x, _ in sorted_items]\n",
    "    ys = [y for _, y in sorted_items]\n",
    "\n",
    "    # 3. Calculate means\n",
    "    n = len(xs)\n",
    "    mean_x = sum(xs) / n\n",
    "    mean_y = sum(ys) / n\n",
    "\n",
    "    # 4. Calculate Slope (m) using Least Squares method\n",
    "    # Formula: m = sum((x - mean_x) * (y - mean_y)) / sum((x - mean_x)^2)\n",
    "    numerator = sum((xi - mean_x) * (yi - mean_y) for xi, yi in zip(xs, ys))\n",
    "    denominator = sum((xi - mean_x) ** 2 for xi in xs)\n",
    "\n",
    "    if denominator == 0:\n",
    "        return 0.0  # Vertical line (all timestamps are the same)\n",
    "\n",
    "    slope = numerator / denominator\n",
    "    return slope\n",
    "\n",
    "\n",
    "def determine_liquidation_risk(row: pd.Series):\n",
    "    predict_transaction_history = {\n",
    "        key: value\n",
    "        for key, value in get_transaction_history_predictions(row).items()\n",
    "        if value\n",
    "    }\n",
    "\n",
    "    is_at_risk = False\n",
    "\n",
    "    most_recent_predictions = predict_transaction_history[\n",
    "        max(predict_transaction_history.keys())\n",
    "    ]\n",
    "    if most_recent_predictions[\"Liquidated\"] >= max(most_recent_predictions.values()):\n",
    "        is_at_risk = True\n",
    "    else:\n",
    "        trend_slopes = {\n",
    "            outcome_event: calculate_trend_slope(\n",
    "                {\n",
    "                    timestamp: preds[outcome_event]\n",
    "                    for timestamp, preds in predict_transaction_history.items()\n",
    "                    if preds and outcome_event in preds\n",
    "                }\n",
    "            )\n",
    "            for outcome_event in predict_transaction_history[-1].keys()\n",
    "        }\n",
    "        if trend_slopes[\"Liquidated\"] >= max(trend_slopes.values()):\n",
    "            is_at_risk = True\n",
    "\n",
    "    return is_at_risk, most_recent_predictions, trend_slopes\n",
    "\n",
    "\n",
    "def optimize_recommendation(row: pd.Series, recommended_action: str):\n",
    "    new_action = row.copy()\n",
    "    new_action[\"timestamp\"] += 600  # Add 10 minute buffer\n",
    "    new_action[\"amount\"] = 10\n",
    "    while determine_liquidation_risk(new_action)[0]:\n",
    "        new_action[\"amount\"] *= 2\n",
    "        print(new_action[\"amount\"])\n",
    "    return new_action\n",
    "\n",
    "\n",
    "def recommend_action(row: pd.Series):\n",
    "    \"\"\"Analyze predicted transaction history to determine\n",
    "    whether the user is currently at risk of liquidation and\n",
    "    provide a simple recommended action. Returns a dictionary\n",
    "    with keys: liquidation_risk, is_at_risk, risk_trend,\n",
    "    recommended_action, reason, details.\"\"\"\n",
    "\n",
    "    is_at_risk, most_recent_predictions, _ = determine_liquidation_risk(row)\n",
    "\n",
    "    recommended_action = (\n",
    "        \"Repay\"\n",
    "        if most_recent_predictions[\"Repay\"] >= most_recent_predictions[\"Deposit\"]\n",
    "        and is_at_risk\n",
    "        else \"Deposit\"\n",
    "    )\n",
    "\n",
    "    return optimize_recommendation(row, recommended_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2038724/510948606.py:14: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  user_history_df = pd.concat(all_events).sort_values(by=\"timestamp\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'numpy.ndarray' and 'Timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(recommendation_cache_file, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     53\u001b[39m         json.dump(recommendations, f)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43mrun_training_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mrun_training_pipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     46\u001b[39m train_set = get_train_set()\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m train_set.iterrows():\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     recommendations[row] = \u001b[43mrecommend_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     50\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(recommendation_cache_file, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mrecommend_action\u001b[39m\u001b[34m(row)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrecommend_action\u001b[39m(row: pd.Series):\n\u001b[32m     86\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Analyze predicted transaction history to determine\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[33;03m    whether the user is currently at risk of liquidation and\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[33;03m    provide a simple recommended action. Returns a dictionary\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[33;03m    with keys: liquidation_risk, is_at_risk, risk_trend,\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[33;03m    recommended_action, reason, details.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     is_at_risk, most_recent_predictions, _ = \u001b[43mdetermine_liquidation_risk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m     recommended_action = (\n\u001b[32m     95\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepay\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     96\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m most_recent_predictions[\u001b[33m\"\u001b[39m\u001b[33mRepay\u001b[39m\u001b[33m\"\u001b[39m] >= most_recent_predictions[\u001b[33m\"\u001b[39m\u001b[33mDeposit\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     97\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m is_at_risk\n\u001b[32m     98\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mDeposit\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     99\u001b[39m     )\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m optimize_recommendation(row, recommended_action)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mdetermine_liquidation_risk\u001b[39m\u001b[34m(row)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdetermine_liquidation_risk\u001b[39m(row: pd.Series):\n\u001b[32m     45\u001b[39m     predict_transaction_history = {\n\u001b[32m     46\u001b[39m         key: value\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[43mget_transaction_history_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m.items()\n\u001b[32m     48\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m value\n\u001b[32m     49\u001b[39m     }\n\u001b[32m     51\u001b[39m     is_at_risk = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     53\u001b[39m     most_recent_predictions = predict_transaction_history[\n\u001b[32m     54\u001b[39m         \u001b[38;5;28mmax\u001b[39m(predict_transaction_history.keys())\n\u001b[32m     55\u001b[39m     ]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mget_transaction_history_predictions\u001b[39m\u001b[34m(row)\u001b[39m\n\u001b[32m     42\u001b[39m index_event = history_row[\u001b[33m\"\u001b[39m\u001b[33mIndex Event\u001b[39m\u001b[33m\"\u001b[39m].title()\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m outcome_event \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[32m     44\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mLiquidated\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     45\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mBorrow\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWithdraw\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     49\u001b[39m ]:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     model = \u001b[43mget_model_for_pair_and_date\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_event\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutcome_event\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     55\u001b[39m         results[history_timestamp][outcome_event] = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mget_model_for_pair_and_date\u001b[39m\u001b[34m(index_event, outcome_event, model_date, verbose)\u001b[39m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos.path.join(DATA_PATH,\u001b[38;5;250m \u001b[39mdataset_path,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mdata.csv\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     50\u001b[39m train_df = pd.read_csv(os.path.join(DATA_PATH, dataset_path, \u001b[33m\"\u001b[39m\u001b[33mdata.csv\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m X_train, y_train, _ = \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# --- Train Model ---\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Prepare target variables for Cox regression\u001b[39;00m\n\u001b[32m     56\u001b[39m y_train_duration = y_train[\u001b[33m\"\u001b[39m\u001b[33mtimeDiff\u001b[39m\u001b[33m\"\u001b[39m].values\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mpreprocess\u001b[39m\u001b[34m(train_df_with_labels, test_features_df, model_date)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m train_df_with_labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_date \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     29\u001b[39m         train_df_with_labels = train_df_with_labels[\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m             \u001b[43m(\u001b[49m\u001b[43mtrain_df_with_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtimestamp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df_with_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtimeDiff\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m            \u001b[49m\u001b[43m<\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_date\u001b[49m\n\u001b[32m     32\u001b[39m         ]\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# Separate features and targets (and drop unneeded columns from features)\u001b[39;00m\n\u001b[32m     35\u001b[39m     target_columns = [\u001b[33m\"\u001b[39m\u001b[33mtimeDiff\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/actionAgent/lib/python3.14/site-packages/pandas/core/ops/common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/actionAgent/lib/python3.14/site-packages/pandas/core/arraylike.py:52\u001b[39m, in \u001b[36mOpsMixin.__le__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__le__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__le__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mle\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/actionAgent/lib/python3.14/site-packages/pandas/core/series.py:6138\u001b[39m, in \u001b[36mSeries._cmp_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   6135\u001b[39m lvalues = \u001b[38;5;28mself\u001b[39m._values\n\u001b[32m   6136\u001b[39m rvalues = extract_array(other, extract_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m6138\u001b[39m res_values = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6140\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._construct_result(res_values, name=res_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/actionAgent/lib/python3.14/site-packages/pandas/core/ops/array_ops.py:330\u001b[39m, in \u001b[36mcomparison_op\u001b[39m\u001b[34m(left, right, op)\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    322\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mLengths must match to compare\u001b[39m\u001b[33m\"\u001b[39m, lvalues.shape, rvalues.shape\n\u001b[32m    323\u001b[39m         )\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m should_extension_dispatch(lvalues, rvalues) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m    326\u001b[39m     (\u001b[38;5;28misinstance\u001b[39m(rvalues, (Timedelta, BaseOffset, Timestamp)) \u001b[38;5;129;01mor\u001b[39;00m right \u001b[38;5;129;01mis\u001b[39;00m NaT)\n\u001b[32m    327\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m lvalues.dtype != \u001b[38;5;28mobject\u001b[39m\n\u001b[32m    328\u001b[39m ):\n\u001b[32m    329\u001b[39m     \u001b[38;5;66;03m# Call the method on lvalues\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     res_values = \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_scalar(rvalues) \u001b[38;5;129;01mand\u001b[39;00m isna(rvalues):  \u001b[38;5;66;03m# TODO: but not pd.NA?\u001b[39;00m\n\u001b[32m    333\u001b[39m     \u001b[38;5;66;03m# numpy does not like comparisons vs None\u001b[39;00m\n\u001b[32m    334\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m operator.ne:\n",
      "\u001b[31mTypeError\u001b[39m: '<=' not supported between instances of 'numpy.ndarray' and 'Timestamp'"
     ]
    }
   ],
   "source": [
    "def get_train_set():\n",
    "    train_set_dir = os.path.join(CACHE_DIR, \"train_set.csv\")\n",
    "    if os.path.exists(train_set_dir):\n",
    "        train_set = pd.read_csv(train_set_dir)\n",
    "    else:\n",
    "        train_set = pd.DataFrame()\n",
    "        train_ranges, test_ranges = get_date_ranges()\n",
    "        min_train_date = train_ranges[0].timestamp()\n",
    "        max_train_date = test_ranges[0].timestamp()\n",
    "        for index_event in EVENTS:\n",
    "            if index_event == \"Liquidated\":\n",
    "                continue\n",
    "            for outcome_event in EVENTS:\n",
    "                with open(\n",
    "                    os.path.join(\n",
    "                        DATA_PATH,\n",
    "                        index_event,\n",
    "                        outcome_event,\n",
    "                        \"data.csv\",\n",
    "                    ),\n",
    "                    \"r\",\n",
    "                ) as f:\n",
    "                    df = pd.read_csv(f)\n",
    "                    train_set = pd.concat(\n",
    "                        [\n",
    "                            train_set,\n",
    "                            df[\n",
    "                                (df[\"timestamp\"] >= min_train_date)\n",
    "                                & (df[\"timestamp\"] < max_train_date)\n",
    "                            ].sample(n=100, random_state=seed),\n",
    "                        ],\n",
    "                        ignore_index=True,\n",
    "                    )\n",
    "        with open(train_set_dir, \"w\") as f:\n",
    "            train_set.to_csv(f, index=False)\n",
    "    return train_set\n",
    "\n",
    "\n",
    "def run_training_pipeline():\n",
    "    recommendation_cache_file = os.path.join(CACHE_DIR, \"recommendations.json\")\n",
    "    if os.path.exists(recommendation_cache_file):\n",
    "        with open(recommendation_cache_file, \"r\") as f:\n",
    "            recommendations = json.load(f)\n",
    "    else:\n",
    "        recommendations = {}\n",
    "    train_set = get_train_set()\n",
    "    for i, row in train_set.iterrows():\n",
    "        recommendations[row] = recommend_action(row)\n",
    "        if i % 10 == 0:\n",
    "            with open(recommendation_cache_file, \"w\") as f:\n",
    "                json.dump(recommendations, f)\n",
    "    with open(recommendation_cache_file, \"w\") as f:\n",
    "        json.dump(recommendations, f)\n",
    "\n",
    "\n",
    "run_training_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "actionAgent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

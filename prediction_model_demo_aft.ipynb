{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "# Install required packages\n",
    "# pip install -q pandas xgboost scikit-learn numpy pyreadr\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Tuple, Optional\n",
    "import pickle as pkl\n",
    "from itertools import chain\n",
    "import pyreadr\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data/\"\n",
    "CACHE_DIR = \"./cache/\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "MODEL_CACHE_DIR = os.path.join(CACHE_DIR, \"models\")\n",
    "os.makedirs(MODEL_CACHE_DIR, exist_ok=True)\n",
    "DATA_CACHE_DIR = os.path.join(CACHE_DIR, \"data\")\n",
    "os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
    "RESULTS_CACHE_DIR = os.path.join(CACHE_DIR, \"results\")\n",
    "os.makedirs(RESULTS_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "EVENTS = [\"Deposit\", \"Withdraw\", \"Repay\", \"Borrow\", \"Liquidated\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    train_df_with_labels: Optional[pd.DataFrame] = None,\n",
    "    test_features_df: Optional[pd.DataFrame] = None,\n",
    "    model_date: Optional[int] = None,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "\n",
    "    # Create unique prefix for saving/loading preprocessing objects\n",
    "    unique_prefix = (\n",
    "        (\n",
    "            present_dataframe := (\n",
    "                train_df_with_labels\n",
    "                if train_df_with_labels is not None\n",
    "                else test_features_df\n",
    "            )\n",
    "        )[\"Index Event\"].iloc[0]\n",
    "        + \"_\"\n",
    "        + present_dataframe[\"Outcome Event\"].iloc[0]\n",
    "        + (f\"_{model_date}_\" if model_date is not None else \"_\")\n",
    "    )\n",
    "    # Define paths for saving/loading preprocessing objects\n",
    "    scaler_path = os.path.join(DATA_CACHE_DIR, unique_prefix + \"scaler.pkl\")\n",
    "    train_cols = os.path.join(DATA_CACHE_DIR, unique_prefix + \"train_cols.pkl\")\n",
    "    top_categories_dict_path = os.path.join(\n",
    "        DATA_CACHE_DIR, unique_prefix + \"top_categories_dict.pkl\"\n",
    "    )\n",
    "\n",
    "    target_columns = [\"timeDiff\", \"status\"]\n",
    "    cols_to_drop = [\n",
    "        \"id\",\n",
    "        \"user\",\n",
    "        \"pool\",\n",
    "        \"Index Event\",\n",
    "        \"Outcome Event\",\n",
    "        \"type\",\n",
    "        \"timestamp\",\n",
    "    ]\n",
    "\n",
    "    if train_df_with_labels is not None:\n",
    "        if model_date is not None:\n",
    "            # model_date may be a pandas.Timestamp while dataframe timestamps are numeric.\n",
    "            # Convert model_date to numeric epoch seconds for a safe comparison.\n",
    "            if isinstance(model_date, pd.Timestamp):\n",
    "                model_date_value = model_date.timestamp()\n",
    "            else:\n",
    "                try:\n",
    "                    model_date_value = float(model_date)\n",
    "                except Exception:\n",
    "                    model_date_value = model_date\n",
    "\n",
    "            train_df_with_labels = train_df_with_labels[\n",
    "                (train_df_with_labels[\"timestamp\"] + train_df_with_labels[\"timeDiff\"])\n",
    "                <= model_date_value\n",
    "            ]\n",
    "\n",
    "        # Separate features and targets (and drop unneeded columns from features)\n",
    "        train_targets = train_df_with_labels[target_columns]\n",
    "        train_features = train_df_with_labels.drop(\n",
    "            columns=target_columns + cols_to_drop, errors=\"ignore\"\n",
    "        )\n",
    "\n",
    "        # Make uncommon categories \"Other\" and one-hot encode categorical features\n",
    "        categorical_cols = train_features.select_dtypes(\n",
    "            include=[\"object\", \"category\"]\n",
    "        ).columns\n",
    "        top_categories_dict = {}\n",
    "        for col in categorical_cols:\n",
    "            if col not in top_categories_dict:\n",
    "                top_categories_dict[col] = (\n",
    "                    train_features[col].value_counts().nlargest(10).index\n",
    "                )\n",
    "            train_features[col] = train_features[col].where(\n",
    "                train_features[col].isin(top_categories_dict[col]), \"Other\"\n",
    "            )\n",
    "        train_features_encoded = pd.get_dummies(\n",
    "            train_features, columns=categorical_cols, dummy_na=True, drop_first=True\n",
    "        )\n",
    "\n",
    "        # Standardize numerical features\n",
    "        numerical_cols = train_features_encoded.select_dtypes(include=np.number).columns\n",
    "        scaler = StandardScaler()\n",
    "        train_features_scaled = scaler.fit_transform(\n",
    "            train_features_encoded[numerical_cols]\n",
    "        )\n",
    "        train_features_final = pd.DataFrame(\n",
    "            train_features_scaled,\n",
    "            index=train_features_encoded.index,\n",
    "            columns=numerical_cols,\n",
    "        ).fillna(0)\n",
    "\n",
    "        # Remove zero-variance columns\n",
    "        cols_to_keep = train_features_final.columns[train_features_final.var() != 0]\n",
    "        train_features_final = train_features_final[cols_to_keep]\n",
    "\n",
    "        # Save preprocessing objects\n",
    "        with open(scaler_path, \"wb\") as f:\n",
    "            pkl.dump(scaler, f)\n",
    "        with open(train_cols, \"wb\") as f:\n",
    "            pkl.dump(train_features_encoded.columns, f)\n",
    "        with open(top_categories_dict_path, \"wb\") as f:\n",
    "            pkl.dump(top_categories_dict, f)\n",
    "\n",
    "    # Process test features if provided\n",
    "    test_processed_features = None\n",
    "    if test_features_df is not None:\n",
    "        test_features = test_features_df.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "        with open(top_categories_dict_path, \"rb\") as f:\n",
    "            top_categories_dict = pkl.load(f)\n",
    "        for col in categorical_cols:\n",
    "            top_categories = top_categories_dict[col]\n",
    "            test_features[col] = test_features[col].where(\n",
    "                test_features[col].isin(top_categories), \"Other\"\n",
    "            )\n",
    "        test_features_encoded = pd.get_dummies(\n",
    "            test_features, columns=categorical_cols, dummy_na=True, drop_first=True\n",
    "        )\n",
    "        with open(train_cols, \"rb\") as f:\n",
    "            train_cols = pkl.load(f)\n",
    "        test_features_aligned = test_features_encoded.reindex(\n",
    "            columns=train_cols, fill_value=0\n",
    "        )\n",
    "        with open(scaler_path, \"rb\") as f:\n",
    "            scaler = pkl.load(f)\n",
    "        test_features_scaled = scaler.transform(test_features_aligned[numerical_cols])\n",
    "        test_features_final = pd.DataFrame(\n",
    "            test_features_scaled,\n",
    "            index=test_features_aligned.index,\n",
    "            columns=numerical_cols,\n",
    "        ).fillna(0)\n",
    "        test_processed_features = test_features_final[cols_to_keep]\n",
    "    return train_features_final, train_targets, test_processed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_for_pair_and_date(\n",
    "    index_event: str,\n",
    "    outcome_event: str,\n",
    "    model_date: int | None = None,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    # normalize model_date for filename\n",
    "    model_date_str = str(model_date) if model_date is not None else \"latest\"\n",
    "    model_filename = f\"xgboost_cox_{index_event}_{outcome_event}_{model_date_str}.ubj\"\n",
    "    model_path = os.path.join(MODEL_CACHE_DIR, model_filename)\n",
    "\n",
    "    # Create model with Cox objective\n",
    "    model = XGBRegressor(\n",
    "        objective=\"survival:cox\",\n",
    "        eval_metric=\"cox-nloglik\",\n",
    "        tree_method=\"hist\",\n",
    "        predictor=\"gpu_predictor\",\n",
    "        device=\"cuda\",\n",
    "        seed=42,\n",
    "        verbosity=0,\n",
    "        max_bin=64,\n",
    "        learning_rate=0.04,\n",
    "        max_depth=5,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.8,\n",
    "        min_child_weight=5,\n",
    "        reg_lambda=1.0,\n",
    "        reg_alpha=0.1,\n",
    "    )\n",
    "\n",
    "    # If model file exists, try to load into the estimator and return the estimator\n",
    "    if os.path.exists(model_path):\n",
    "        if verbose:\n",
    "            print(f\"Loading existing model from {model_path}\")\n",
    "        try:\n",
    "            model.load_model(model_path)\n",
    "            if verbose:\n",
    "                print(f\"model loaded from {model_path}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Warning: failed to load model from {model_path}: {e}. Will retrain.\"\n",
    "            )\n",
    "\n",
    "    dataset_path = os.path.join(index_event, outcome_event)\n",
    "\n",
    "    # --- Load and Preprocess ---\n",
    "    if verbose:\n",
    "        print(f\"Loading data from {os.path.join(DATA_PATH, dataset_path, 'data.csv')}\")\n",
    "    train_df = pd.read_csv(os.path.join(DATA_PATH, dataset_path, \"data.csv\"))\n",
    "\n",
    "    X_train, y_train, _ = preprocess(train_df, model_date=model_date)\n",
    "\n",
    "    # --- Train Model ---\n",
    "    # Prepare target variables for Cox regression\n",
    "    y_train_duration = y_train[\"timeDiff\"].values\n",
    "    y_train_event = y_train[\"status\"].values\n",
    "\n",
    "    # Fit model: XGBoost Cox expects labels to be the event indicators\n",
    "    # and the sample_weight to be the durations\n",
    "    if verbose:\n",
    "        print(\"Training model...\")\n",
    "    try:\n",
    "        model.fit(X_train, y_train_event, sample_weight=y_train_duration)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Model training failed for {dataset_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Save model: try estimator's save_model, fall back to Booster.save_model\n",
    "    try:\n",
    "        # XGBRegressor implements save_model; call it and confirm file created\n",
    "        model.save_model(model_path)\n",
    "        if verbose:\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            booster = model.get_booster()\n",
    "            booster.save_model(model_path)\n",
    "            if verbose:\n",
    "                print(f\"Model booster saved to {model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to save model to {model_path}: {e}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_for_all_event_pairs(\n",
    "    model_date: int | None = None, verbose: bool = False\n",
    "):\n",
    "    # Define all 16 event pairs\n",
    "    index_events = EVENTS\n",
    "    outcome_events = index_events\n",
    "    event_pairs = [\n",
    "        event_pair\n",
    "        for sub_event_pairs in [\n",
    "            [(index_event, outcome_event) for outcome_event in outcome_events]\n",
    "            for index_event in index_events\n",
    "        ]\n",
    "        for event_pair in sub_event_pairs\n",
    "    ]\n",
    "\n",
    "    for index_event, outcome_event in event_pairs:\n",
    "        if index_event == outcome_event and index_event == \"Liquidated\":\n",
    "            continue\n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Training for: {index_event} -> {outcome_event}\")\n",
    "            print(f\"{'='*50}\")\n",
    "\n",
    "        get_model_for_pair_and_date(\n",
    "            index_event, outcome_event, model_date=model_date, verbose=verbose\n",
    "        )\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n\\nAll prediction files have been generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_ranges():\n",
    "    if os.path.exists(os.path.join(CACHE_DIR, \"date_ranges.pkl\")):\n",
    "        with open(os.path.join(CACHE_DIR, \"date_ranges.pkl\"), \"rb\") as f:\n",
    "            return pkl.load(f)\n",
    "    transactions_df = pyreadr.read_r(\"./data/transactions.rds\")[None]\n",
    "    min_date = transactions_df[\"timestamp\"].min() * 1e9\n",
    "    print(min_date)\n",
    "    max_date = transactions_df[\"timestamp\"].max() * 1e9\n",
    "    print(max_date)\n",
    "    train_start_date = min_date + 0.4 * (max_date - min_date)\n",
    "    print(train_start_date)\n",
    "    test_start_date = min_date + 0.8 * (max_date - min_date)\n",
    "    print(test_start_date)\n",
    "    train_dates = pd.date_range(start=train_start_date, end=test_start_date, freq=\"2W\")\n",
    "    test_dates = pd.date_range(start=test_start_date, end=max_date, freq=\"2W\")\n",
    "    with open(os.path.join(CACHE_DIR, \"date_ranges.pkl\"), \"wb\") as f:\n",
    "        pkl.dump((train_dates, test_dates), f)\n",
    "    return train_dates, test_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for date in chain(*get_date_ranges()):\n",
    "#     train_models_for_all_event_pairs(model_date=date.timestamp(), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_history(user_id: str, up_to_timestamp: int) -> pd.DataFrame:\n",
    "    all_events = []\n",
    "    for index_event in EVENTS:\n",
    "        for outcome_event in EVENTS:\n",
    "            event_path = os.path.join(DATA_PATH, index_event, outcome_event, \"data.csv\")\n",
    "            if os.path.exists(event_path):\n",
    "                event_df = pd.read_csv(event_path)\n",
    "                user_events = event_df[\n",
    "                    (event_df[\"user\"] == user_id)\n",
    "                    & (event_df[\"timestamp\"] <= up_to_timestamp)\n",
    "                ]\n",
    "                if len(user_events):\n",
    "                    all_events.append(user_events)\n",
    "    if all_events:\n",
    "        user_history_df = pd.concat(all_events).sort_values(by=\"timestamp\")\n",
    "    else:\n",
    "        user_history_df = pd.DataFrame()\n",
    "    return user_history_df\n",
    "\n",
    "\n",
    "def get_transaction_history_predictions(row: pd.Series) -> pd.DataFrame:\n",
    "    results_cache_file = (\n",
    "        RESULTS_CACHE_DIR + f\"{row['user']}_{row['timestamp']}_{row['amount']}.pkl\"\n",
    "    )\n",
    "    if os.path.exists(results_cache_file):\n",
    "        with open(results_cache_file, \"rb\") as f:\n",
    "            return pkl.load(f)\n",
    "\n",
    "    results = {}\n",
    "    train_dates, test_dates = get_date_ranges()\n",
    "    dates = train_dates.union(test_dates)\n",
    "\n",
    "    user_history = get_user_history(\n",
    "        user_id=row[\"user\"], up_to_timestamp=row[\"timestamp\"]\n",
    "    )\n",
    "\n",
    "    model_date = dates[dates <= pd.to_datetime(row[\"timestamp\"], unit=\"s\")].max()\n",
    "    for _, history_row in user_history.iterrows():\n",
    "        history_timestamp = history_row[\"timestamp\"]\n",
    "        results[history_timestamp] = {}\n",
    "        history_row = history_row.copy()\n",
    "\n",
    "        index_event = history_row[\"Index Event\"].title()\n",
    "        for outcome_event in [\n",
    "            \"Liquidated\",\n",
    "            \"Borrow\",\n",
    "            \"Deposit\",\n",
    "            \"Repay\",\n",
    "            \"Withdraw\",\n",
    "        ]:\n",
    "            model = get_model_for_pair_and_date(\n",
    "                index_event, outcome_event, model_date=model_date, verbose=True\n",
    "            )\n",
    "\n",
    "            if model is None:\n",
    "                results[history_timestamp][outcome_event] = None\n",
    "                continue\n",
    "\n",
    "            history_row[\"Outcome Event\"] = outcome_event.lower()\n",
    "            _, _, test_features = preprocess(\n",
    "                test_features_df=history_row.to_frame().T,\n",
    "                model_date=model_date,\n",
    "            )\n",
    "\n",
    "            prediction = model.predict(test_features)\n",
    "            results[history_timestamp][outcome_event] = prediction[0]\n",
    "\n",
    "    with open(\n",
    "        results_cache_file,\n",
    "        \"wb\",\n",
    "    ) as f:\n",
    "        pkl.dump(results, f)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trend_slope(data):\n",
    "    \"\"\"\n",
    "    Calculates the linear regression slope of a dataset to determine the trend.\n",
    "\n",
    "    Args:\n",
    "        data (dict): A dictionary where keys are timestamps (int/float)\n",
    "                     and values are numbers.\n",
    "\n",
    "    Returns:\n",
    "        float: The slope of the trend line.\n",
    "               > 0 means increasing, < 0 means decreasing.\n",
    "               Returns 0.0 if not enough data.\n",
    "    \"\"\"\n",
    "    if len(data) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    # 1. Sort data by timestamp (keys)\n",
    "    sorted_items = sorted(data.items())\n",
    "\n",
    "    # 2. Extract x (timestamps) and y (values)\n",
    "    # We subtract the first timestamp from all x values to normalize them\n",
    "    # (starts at time 0). This prevents precision errors with large timestamps.\n",
    "    start_time = sorted_items[0][0]\n",
    "    xs = [x - start_time for x, _ in sorted_items]\n",
    "    ys = [y for _, y in sorted_items]\n",
    "\n",
    "    # 3. Calculate means\n",
    "    n = len(xs)\n",
    "    mean_x = sum(xs) / n\n",
    "    mean_y = sum(ys) / n\n",
    "\n",
    "    # 4. Calculate Slope (m) using Least Squares method\n",
    "    # Formula: m = sum((x - mean_x) * (y - mean_y)) / sum((x - mean_x)^2)\n",
    "    numerator = sum((xi - mean_x) * (yi - mean_y) for xi, yi in zip(xs, ys))\n",
    "    denominator = sum((xi - mean_x) ** 2 for xi in xs)\n",
    "\n",
    "    if denominator == 0:\n",
    "        return 0.0  # Vertical line (all timestamps are the same)\n",
    "\n",
    "    slope = numerator / denominator\n",
    "    return slope\n",
    "\n",
    "\n",
    "def determine_liquidation_risk(row: pd.Series):\n",
    "    predict_transaction_history = {\n",
    "        key: value\n",
    "        for key, value in get_transaction_history_predictions(row).items()\n",
    "        if value\n",
    "    }\n",
    "\n",
    "    is_at_risk = False\n",
    "\n",
    "    most_recent_predictions = predict_transaction_history[\n",
    "        max(predict_transaction_history.keys())\n",
    "    ]\n",
    "    if most_recent_predictions[\"Liquidated\"] >= max(most_recent_predictions.values()):\n",
    "        is_at_risk = True\n",
    "    else:\n",
    "        trend_slopes = {\n",
    "            outcome_event: calculate_trend_slope(\n",
    "                {\n",
    "                    timestamp: preds[outcome_event]\n",
    "                    for timestamp, preds in predict_transaction_history.items()\n",
    "                    if preds and outcome_event in preds\n",
    "                }\n",
    "            )\n",
    "            for outcome_event in predict_transaction_history[-1].keys()\n",
    "        }\n",
    "        if trend_slopes[\"Liquidated\"] >= max(trend_slopes.values()):\n",
    "            is_at_risk = True\n",
    "\n",
    "    return is_at_risk, most_recent_predictions, trend_slopes\n",
    "\n",
    "\n",
    "def optimize_recommendation(row: pd.Series, recommended_action: str):\n",
    "    new_action = row.copy()\n",
    "    new_action[\"timestamp\"] += 600  # Add 10 minute buffer\n",
    "    new_action[\"amount\"] = 10\n",
    "    while determine_liquidation_risk(new_action)[0]:\n",
    "        new_action[\"amount\"] *= 2\n",
    "        print(new_action[\"amount\"])\n",
    "    return new_action\n",
    "\n",
    "\n",
    "def recommend_action(row: pd.Series):\n",
    "    \"\"\"Analyze predicted transaction history to determine\n",
    "    whether the user is currently at risk of liquidation and\n",
    "    provide a simple recommended action. Returns a dictionary\n",
    "    with keys: liquidation_risk, is_at_risk, risk_trend,\n",
    "    recommended_action, reason, details.\"\"\"\n",
    "\n",
    "    is_at_risk, most_recent_predictions, _ = determine_liquidation_risk(row)\n",
    "\n",
    "    recommended_action = (\n",
    "        \"Repay\"\n",
    "        if most_recent_predictions[\"Repay\"] >= most_recent_predictions[\"Deposit\"]\n",
    "        and is_at_risk\n",
    "        else \"Deposit\"\n",
    "    )\n",
    "\n",
    "    return optimize_recommendation(row, recommended_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ./data/Deposit/Liquidated/data.csv\n",
      "Training model...\n",
      "ERROR: Model training failed for Deposit/Liquidated: [13:33:48] /workspace/src/common/device_vector.cu:23: Memory allocation error on worker 0: std::bad_alloc: cudaErrorMemoryAllocation: out of memory\n",
      "- Free memory: 24.9375MB\n",
      "- Requested memory: 121.305MB\n",
      "\n",
      "Stack trace:\n",
      "  [bt] (0) /home/spadef/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/lib/libxgboost.so(+0x2bdf8c) [0x753d474bdf8c]\n",
      "  [bt] (1) /home/spadef/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/lib/libxgboost.so(+0xaf9aa3) [0x753d47cf9aa3]\n",
      "  [bt] (2) /home/spadef/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/lib/libxgboost.so(+0xcb17cb) [0x753d47eb17cb]\n",
      "  [bt] (3) /home/spadef/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/lib/libxgboost.so(+0xcb9716) [0x753d47eb9716]\n",
      "  [bt] (4) /home/spadef/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/lib/libxgboost.so(+0xc9e5c2) [0x753d47e9e5c2]\n",
      "  [bt] (5) /home/spadef/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/lib/libxgboost.so(+0xca0e4f) [0x753d47ea0e4f]\n",
      "  [bt] (6) /home/spadef/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/lib/libxgboost.so(+0xd2095f) [0x753d47f2095f]\n",
      "  [bt] (7) /home/spadef/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/lib/libxgboost.so(+0x115c8c9) [0x753d4835c8c9]\n",
      "  [bt] (8) /home/spadef/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/lib/libxgboost.so(+0x1190663) [0x753d48390663]\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[13:33:48] /workspace/src/common/device_vector.cu:23: Memory allocation error on worker 0: std::bad_alloc: cudaErrorMemoryAllocation: out of memory\n- Free memory: 24.9375MB\n- Requested memory: 121.305MB\n\nStack trace:\n  [bt] (0) /home/spadef/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/lib/libxgboost.so(+0x2bdf8c) [0x753d474bdf8c]\n  [bt] (1) /home/spadef/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/lib/libxgboost.so(+0xaf9aa3) [0x753d47cf9aa3]\n  [bt] (2) /home/spadef/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/lib/libxgboost.so(+0xcb17cb) [0x753d47eb17cb]\n  [bt] (3) /home/spadef/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/lib/libxgboost.so(+0xcb9716) [0x753d47eb9716]\n  [bt] (4) /home/spadef/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/lib/libxgboost.so(+0xc9e5c2) [0x753d47e9e5c2]\n  [bt] (5) /home/spadef/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/lib/libxgboost.so(+0xca0e4f) [0x753d47ea0e4f]\n  [bt] (6) /home/spadef/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/lib/libxgboost.so(+0xd2095f) [0x753d47f2095f]\n  [bt] (7) /home/spadef/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/lib/libxgboost.so(+0x115c8c9) [0x753d4835c8c9]\n  [bt] (8) /home/spadef/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/lib/libxgboost.so(+0x1190663) [0x753d48390663]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mXGBoostError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[122]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(recommendation_cache_file, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     53\u001b[39m         json.dump(recommendations, f)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43mrun_training_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[122]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mrun_training_pipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     46\u001b[39m train_set = get_train_set()\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m train_set.iterrows():\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     recommendations[row] = \u001b[43mrecommend_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     50\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(recommendation_cache_file, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[121]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mrecommend_action\u001b[39m\u001b[34m(row)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrecommend_action\u001b[39m(row: pd.Series):\n\u001b[32m     86\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Analyze predicted transaction history to determine\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[33;03m    whether the user is currently at risk of liquidation and\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[33;03m    provide a simple recommended action. Returns a dictionary\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[33;03m    with keys: liquidation_risk, is_at_risk, risk_trend,\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[33;03m    recommended_action, reason, details.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     is_at_risk, most_recent_predictions, _ = \u001b[43mdetermine_liquidation_risk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m     recommended_action = (\n\u001b[32m     95\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepay\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     96\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m most_recent_predictions[\u001b[33m\"\u001b[39m\u001b[33mRepay\u001b[39m\u001b[33m\"\u001b[39m] >= most_recent_predictions[\u001b[33m\"\u001b[39m\u001b[33mDeposit\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     97\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m is_at_risk\n\u001b[32m     98\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mDeposit\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     99\u001b[39m     )\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m optimize_recommendation(row, recommended_action)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[121]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mdetermine_liquidation_risk\u001b[39m\u001b[34m(row)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdetermine_liquidation_risk\u001b[39m(row: pd.Series):\n\u001b[32m     45\u001b[39m     predict_transaction_history = {\n\u001b[32m     46\u001b[39m         key: value\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[43mget_transaction_history_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m.items()\n\u001b[32m     48\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m value\n\u001b[32m     49\u001b[39m     }\n\u001b[32m     51\u001b[39m     is_at_risk = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     53\u001b[39m     most_recent_predictions = predict_transaction_history[\n\u001b[32m     54\u001b[39m         \u001b[38;5;28mmax\u001b[39m(predict_transaction_history.keys())\n\u001b[32m     55\u001b[39m     ]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[120]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mget_transaction_history_predictions\u001b[39m\u001b[34m(row)\u001b[39m\n\u001b[32m     43\u001b[39m index_event = history_row[\u001b[33m\"\u001b[39m\u001b[33mIndex Event\u001b[39m\u001b[33m\"\u001b[39m].title()\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m outcome_event \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[32m     45\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mLiquidated\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     46\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mBorrow\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWithdraw\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     50\u001b[39m ]:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     model = \u001b[43mget_model_for_pair_and_date\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_event\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutcome_event\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     56\u001b[39m         results[history_timestamp][outcome_event] = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[116]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mget_model_for_pair_and_date\u001b[39m\u001b[34m(index_event, outcome_event, model_date, verbose)\u001b[39m\n\u001b[32m     62\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_event\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_train_duration\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mERROR: Model training failed for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/sklearn.py:1368\u001b[39m, in \u001b[36mXGBModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1366\u001b[39m     obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[38;5;28mself\u001b[39m._set_evaluation_result(evals_result)\n\u001b[32m   1383\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/training.py:199\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/core.py:2433\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2430\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2432\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2433\u001b[39m     \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2434\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2435\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2436\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2437\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2438\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2439\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/core.py:323\u001b[39m, in \u001b[36m_check_call\u001b[39m\u001b[34m(ret)\u001b[39m\n\u001b[32m    312\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[32m    313\u001b[39m \n\u001b[32m    314\u001b[39m \u001b[33;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m    return value from API calls\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret != \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "\u001b[31mXGBoostError\u001b[39m: [13:33:48] /workspace/src/common/device_vector.cu:23: Memory allocation error on worker 0: std::bad_alloc: cudaErrorMemoryAllocation: out of memory\n- Free memory: 24.9375MB\n- Requested memory: 121.305MB\n\nStack trace:\n  [bt] (0) /home/spadef/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/lib/libxgboost.so(+0x2bdf8c) [0x753d474bdf8c]\n  [bt] (1) /home/spadef/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/lib/libxgboost.so(+0xaf9aa3) [0x753d47cf9aa3]\n  [bt] (2) /home/spadef/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/lib/libxgboost.so(+0xcb17cb) [0x753d47eb17cb]\n  [bt] (3) /home/spadef/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/lib/libxgboost.so(+0xcb9716) [0x753d47eb9716]\n  [bt] (4) /home/spadef/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/lib/libxgboost.so(+0xc9e5c2) [0x753d47e9e5c2]\n  [bt] (5) /home/spadef/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/lib/libxgboost.so(+0xca0e4f) [0x753d47ea0e4f]\n  [bt] (6) /home/spadef/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/lib/libxgboost.so(+0xd2095f) [0x753d47f2095f]\n  [bt] (7) /home/spadef/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/lib/libxgboost.so(+0x115c8c9) [0x753d4835c8c9]\n  [bt] (8) /home/spadef/miniconda3/envs/actionAgent/lib/python3.14/site-packages/xgboost/lib/libxgboost.so(+0x1190663) [0x753d48390663]\n\n"
     ]
    }
   ],
   "source": [
    "def get_train_set():\n",
    "    train_set_dir = os.path.join(CACHE_DIR, \"train_set.csv\")\n",
    "    if os.path.exists(train_set_dir):\n",
    "        train_set = pd.read_csv(train_set_dir)\n",
    "    else:\n",
    "        train_set = pd.DataFrame()\n",
    "        train_ranges, test_ranges = get_date_ranges()\n",
    "        min_train_date = train_ranges[0].timestamp()\n",
    "        max_train_date = test_ranges[0].timestamp()\n",
    "        for index_event in EVENTS:\n",
    "            if index_event == \"Liquidated\":\n",
    "                continue\n",
    "            for outcome_event in EVENTS:\n",
    "                with open(\n",
    "                    os.path.join(\n",
    "                        DATA_PATH,\n",
    "                        index_event,\n",
    "                        outcome_event,\n",
    "                        \"data.csv\",\n",
    "                    ),\n",
    "                    \"r\",\n",
    "                ) as f:\n",
    "                    df = pd.read_csv(f)\n",
    "                    train_set = pd.concat(\n",
    "                        [\n",
    "                            train_set,\n",
    "                            df[\n",
    "                                (df[\"timestamp\"] >= min_train_date)\n",
    "                                & (df[\"timestamp\"] < max_train_date)\n",
    "                            ].sample(n=100, random_state=seed),\n",
    "                        ],\n",
    "                        ignore_index=True,\n",
    "                    )\n",
    "        with open(train_set_dir, \"w\") as f:\n",
    "            train_set.to_csv(f, index=False)\n",
    "    return train_set\n",
    "\n",
    "\n",
    "def run_training_pipeline():\n",
    "    recommendation_cache_file = os.path.join(CACHE_DIR, \"recommendations.json\")\n",
    "    if os.path.exists(recommendation_cache_file):\n",
    "        with open(recommendation_cache_file, \"r\") as f:\n",
    "            recommendations = json.load(f)\n",
    "    else:\n",
    "        recommendations = {}\n",
    "    train_set = get_train_set()\n",
    "    for i, row in train_set.iterrows():\n",
    "        recommendations[row] = recommend_action(row)\n",
    "        if i % 10 == 0:\n",
    "            with open(recommendation_cache_file, \"w\") as f:\n",
    "                json.dump(recommendations, f)\n",
    "    with open(recommendation_cache_file, \"w\") as f:\n",
    "        json.dump(recommendations, f)\n",
    "\n",
    "\n",
    "run_training_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "actionAgent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

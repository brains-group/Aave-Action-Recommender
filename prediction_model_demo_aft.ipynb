{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FinSurvival Competition: Starter Notebook (AFT Model Prediction Submission)\n",
    "\n",
    "**Objective:** This notebook provides a workflow for creating a valid prediction submission using the `WeibullAFTFitter` model. The competition requires you to submit a `.zip` file containing 16 separate prediction files in CSV format.\n",
    "\n",
    "This notebook will guide you through:\n",
    "1.  Loading the training and test sets for each of the 16 tasks from a single directory.\n",
    "2.  Training a model (using `WeibullAFTFitter` as an example).\n",
    "3.  Generating predictions on the test set in the required format.\n",
    "4.  Saving each set of predictions to a correctly named CSV file.\n",
    "5.  Zipping all 16 prediction files for submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# pip install -q pandas lifelines==0.27.8 scikit-learn==1.2.2 scikit-survival==0.21.0 numpy\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from lifelines import WeibullAFTFitter\n",
    "from lifelines.exceptions import ConvergenceError\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Tuple, Optional\n",
    "from utils.constants import *\n",
    "import pickle as pkl\n",
    "import xgboost as xgb\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from utils.model_training import preprocess, get_best_params\n",
    "\n",
    "# Module-level cache for loaded/trained models to reuse across calls\n",
    "MODELS_CACHE: dict = {}\n",
    "\n",
    "# Module-level cache for preprocessing artifacts (scaler, columns, categories)\n",
    "PREPROCESS_CACHE: dict = {}\n",
    "\n",
    "def get_concordance_index(\n",
    "    test_df: pd.DataFrame, \n",
    "    predictions: np.ndarray\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculates the concordance index for survival models using scikit-survival.\n",
    "    Replaces any NaN predictions with -1.\n",
    "    \"\"\"\n",
    "    # Replace NaN predictions with a value representing the worst possible score (shortest survival)\n",
    "    # Using -1 is a robust way to handle failed predictions without causing numerical errors.\n",
    "    predictions[np.isnan(predictions)] = -1\n",
    "    \n",
    "    event_indicator = test_df['status'].astype(bool)\n",
    "    event_time = test_df['timeDiff']\n",
    "\n",
    "    # Handle cases where all events are censored or all are non-censored in the test set\n",
    "    if len(np.unique(event_indicator)) == 1:\n",
    "        return 0.5  # Return a neutral score\n",
    "\n",
    "    c_index, _, _, _, _ = concordance_index_censored(\n",
    "        event_indicator, event_time, -predictions\n",
    "    )\n",
    "    \n",
    "    return c_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Loop, Train, and Save Predictions\n",
    "\n",
    "This is the main part of the notebook. We will loop through all 16 tasks. For each task, we will:\n",
    "1. Load the training data and the test features.\n",
    "2. Preprocess both.\n",
    "3. Train a model on the training data.\n",
    "4. Generate predictions on the processed test features.\n",
    "5. Save the predictions to a CSV file with the correct name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Borrow -> Borrow\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spadef/.local/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [20:09:36] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-cox-nloglik:13.02686\n",
      "[100]\ttrain-cox-nloglik:12.46353\n",
      "[200]\ttrain-cox-nloglik:12.43498\n",
      "[300]\ttrain-cox-nloglik:12.42362\n",
      "[400]\ttrain-cox-nloglik:12.41585\n",
      "[500]\ttrain-cox-nloglik:12.41030\n",
      "[600]\ttrain-cox-nloglik:12.40585\n",
      "[700]\ttrain-cox-nloglik:12.40168\n",
      "[800]\ttrain-cox-nloglik:12.39794\n",
      "[900]\ttrain-cox-nloglik:12.39456\n",
      "[999]\ttrain-cox-nloglik:12.39122\n",
      "Generating predictions for Borrow/Borrow...\n",
      "Calculating Concordance Index...\n",
      "Concordance index: 0.7580991696462193\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Borrow -> Deposit\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spadef/.local/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [20:14:24] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-cox-nloglik:13.34857\n",
      "[100]\ttrain-cox-nloglik:12.62611\n",
      "[200]\ttrain-cox-nloglik:12.52044\n",
      "[300]\ttrain-cox-nloglik:12.45962\n",
      "[400]\ttrain-cox-nloglik:12.41861\n",
      "[500]\ttrain-cox-nloglik:12.38611\n",
      "[600]\ttrain-cox-nloglik:12.35869\n",
      "[700]\ttrain-cox-nloglik:12.33761\n",
      "[800]\ttrain-cox-nloglik:12.31973\n",
      "[900]\ttrain-cox-nloglik:12.30231\n",
      "[999]\ttrain-cox-nloglik:12.28820\n",
      "Generating predictions for Borrow/Deposit...\n",
      "Calculating Concordance Index...\n",
      "Concordance index: 0.7976878511868747\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Borrow -> Repay\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spadef/.local/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [20:17:35] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-cox-nloglik:12.90863\n",
      "[100]\ttrain-cox-nloglik:12.09682\n",
      "[200]\ttrain-cox-nloglik:12.04188\n",
      "[300]\ttrain-cox-nloglik:12.02712\n",
      "[400]\ttrain-cox-nloglik:12.01904\n",
      "[500]\ttrain-cox-nloglik:12.01326\n",
      "[600]\ttrain-cox-nloglik:12.00856\n",
      "[700]\ttrain-cox-nloglik:12.00425\n",
      "[800]\ttrain-cox-nloglik:12.00021\n",
      "[900]\ttrain-cox-nloglik:11.99628\n",
      "[999]\ttrain-cox-nloglik:11.99286\n",
      "Generating predictions for Borrow/Repay...\n",
      "Calculating Concordance Index...\n",
      "Concordance index: 0.8316366275053256\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Borrow -> Withdraw\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spadef/.local/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [20:21:07] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-cox-nloglik:13.36746\n",
      "[100]\ttrain-cox-nloglik:12.51423\n",
      "[200]\ttrain-cox-nloglik:12.42420\n",
      "[300]\ttrain-cox-nloglik:12.36258\n",
      "[400]\ttrain-cox-nloglik:12.31621\n",
      "[500]\ttrain-cox-nloglik:12.28470\n",
      "[600]\ttrain-cox-nloglik:12.25637\n",
      "[700]\ttrain-cox-nloglik:12.23250\n",
      "[800]\ttrain-cox-nloglik:12.21064\n",
      "[900]\ttrain-cox-nloglik:12.19166\n",
      "[999]\ttrain-cox-nloglik:12.17420\n",
      "Generating predictions for Borrow/Withdraw...\n",
      "Calculating Concordance Index...\n",
      "Concordance index: 0.7893704199101154\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Borrow -> Liquidated\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spadef/.local/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [20:23:53] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-cox-nloglik:12.99286\n",
      "[100]\ttrain-cox-nloglik:11.48899\n",
      "[200]\ttrain-cox-nloglik:11.13069\n",
      "[300]\ttrain-cox-nloglik:10.89325\n",
      "[400]\ttrain-cox-nloglik:10.69311\n",
      "[500]\ttrain-cox-nloglik:10.52992\n",
      "[600]\ttrain-cox-nloglik:10.40198\n",
      "[700]\ttrain-cox-nloglik:10.28913\n",
      "[800]\ttrain-cox-nloglik:10.19518\n",
      "[900]\ttrain-cox-nloglik:10.11120\n",
      "[999]\ttrain-cox-nloglik:10.04227\n",
      "Generating predictions for Borrow/Liquidated...\n",
      "Calculating Concordance Index...\n",
      "Concordance index: 0.7793637427524134\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Deposit -> Borrow\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spadef/.local/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [20:25:15] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-cox-nloglik:14.25664\n",
      "[100]\ttrain-cox-nloglik:13.09202\n",
      "[200]\ttrain-cox-nloglik:13.00774\n",
      "[300]\ttrain-cox-nloglik:12.95082\n",
      "[400]\ttrain-cox-nloglik:12.91094\n",
      "[500]\ttrain-cox-nloglik:12.88164\n",
      "[600]\ttrain-cox-nloglik:12.85799\n",
      "[700]\ttrain-cox-nloglik:12.83585\n",
      "[800]\ttrain-cox-nloglik:12.81678\n",
      "[900]\ttrain-cox-nloglik:12.79684\n",
      "[999]\ttrain-cox-nloglik:12.78206\n",
      "Generating predictions for Deposit/Borrow...\n",
      "Calculating Concordance Index...\n",
      "Concordance index: 0.8977365094986042\n",
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Deposit -> Deposit\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spadef/.local/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [20:34:38] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-cox-nloglik:13.87636\n",
      "[100]\ttrain-cox-nloglik:13.14358\n",
      "[200]\ttrain-cox-nloglik:13.10512\n",
      "[300]\ttrain-cox-nloglik:13.09325\n",
      "[400]\ttrain-cox-nloglik:13.08671\n",
      "[500]\ttrain-cox-nloglik:13.08099\n",
      "[600]\ttrain-cox-nloglik:13.07662\n",
      "[700]\ttrain-cox-nloglik:13.07295\n",
      "[800]\ttrain-cox-nloglik:13.06954\n",
      "[900]\ttrain-cox-nloglik:13.06669\n",
      "[999]\ttrain-cox-nloglik:13.06428\n",
      "Generating predictions for Deposit/Deposit...\n",
      "Calculating Concordance Index...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 62\u001b[39m\n\u001b[32m     58\u001b[39m     predictions = -model.predict(X_test_processed)\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCalculating Concordance Index...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConcordance index: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mget_concordance_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_references_df\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     63\u001b[39m     )\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ConvergenceError, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     67\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mERROR: The model for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m failed to train. No prediction file will be created.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     68\u001b[39m     )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mget_concordance_index\u001b[39m\u001b[34m(test_df, predictions)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np.unique(event_indicator)) == \u001b[32m1\u001b[39m:\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0.5\u001b[39m  \u001b[38;5;66;03m# Return a neutral score\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m c_index, _, _, _, _ = \u001b[43mconcordance_index_censored\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevent_indicator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43mpredictions\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m c_index\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sksurv/metrics.py:220\u001b[39m, in \u001b[36mconcordance_index_censored\u001b[39m\u001b[34m(event_indicator, event_time, estimate, tied_tol)\u001b[39m\n\u001b[32m    216\u001b[39m event_indicator, event_time, estimate = _check_inputs(event_indicator, event_time, estimate)\n\u001b[32m    218\u001b[39m w = np.ones_like(estimate)\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_estimate_concordance_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_indicator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtied_tol\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sksurv/metrics.py:134\u001b[39m, in \u001b[36m_estimate_concordance_index\u001b[39m\u001b[34m(event_indicator, event_time, estimate, weights, tied_tol)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# an event should have a higher score\u001b[39;00m\n\u001b[32m    133\u001b[39m con = est < est_i\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m n_con = \u001b[43mcon\u001b[49m\u001b[43m[\u001b[49m\u001b[43m~\u001b[49m\u001b[43mties\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m numerator += w_i * n_con + \u001b[32m0.5\u001b[39m * w_i * n_ties\n\u001b[32m    137\u001b[39m denominator += w_i * mask.sum()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/finsur2/lib/python3.12/site-packages/numpy/_core/_methods.py:51\u001b[39m, in \u001b[36m_sum\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sum\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     50\u001b[39m          initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Define path to the single participant data folder.\n",
    "PARTICIPANT_DATA_PATH = \"./data/\"\n",
    "\n",
    "# Define all 16 event pairs\n",
    "index_events = [\"Borrow\", \"Deposit\", \"Repay\", \"Withdraw\", \"Liquidated\"]\n",
    "outcome_events = index_events\n",
    "event_pairs = [\n",
    "    (index_event, outcome_event)\n",
    "    for index_event in index_events\n",
    "    for outcome_event in outcome_events\n",
    "]\n",
    "\n",
    "for index_event, outcome_event in event_pairs:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing and Predicting for: {index_event} -> {outcome_event}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    dataset_path = os.path.join(index_event, outcome_event)\n",
    "\n",
    "    # --- Load and Preprocess ---\n",
    "    try:\n",
    "        data_df = pd.read_csv(\n",
    "            os.path.join(PARTICIPANT_DATA_PATH, dataset_path, \"data.csv\")\n",
    "        )\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Data not found for {dataset_path}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    if data_df is None or data_df.shape[0] == 0:\n",
    "        continue\n",
    "\n",
    "    buffer_duration = 30 * 60 * 60 * 24\n",
    "    train_cutoff = 1722526142 - buffer_duration\n",
    "\n",
    "    train_df = data_df[data_df[\"timestamp\"] <= train_cutoff]\n",
    "    test_df = data_df[data_df[\"timestamp\"] > train_cutoff]\n",
    "    reference_cols = [\"timeDiff\", \"status\"]\n",
    "    feature_cols = [col for col in train_df.columns if col not in reference_cols]\n",
    "    test_features_df = test_df[feature_cols]\n",
    "    test_references_df = test_df[reference_cols]\n",
    "\n",
    "    X_train, y_train, X_test_processed, _ = preprocess(train_df, test_features_df)\n",
    "\n",
    "    # --- Train Model ---\n",
    "    try:\n",
    "        params = {\n",
    "            \"objective\": \"survival:cox\",\n",
    "            \"eval_metric\": \"cox-nloglik\",\n",
    "            \"device\": \"cuda\",\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"device\": \"cuda\",\n",
    "            \"seed\": seed,\n",
    "            \"verbosity\": 1,\n",
    "            \"max_bin\": 64,\n",
    "            \"learning_rate\": 0.04,\n",
    "            \"max_depth\": 5,\n",
    "            \"subsample\": 0.85,\n",
    "            \"colsample_bytree\": 0.8,\n",
    "            \"min_child_weight\": 5,\n",
    "            \"reg_lambda\": 1.0,\n",
    "            \"reg_alpha\": 0.1,\n",
    "        }\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            X_train,\n",
    "            num_boost_round=1000,\n",
    "            evals=[(X_train, \"train\")],\n",
    "            verbose_eval=100,\n",
    "        )\n",
    "\n",
    "        # --- Generate and Save Predictions ---\n",
    "        print(f\"Generating predictions for {dataset_path}...\")\n",
    "        # Use the processed test features to make predictions\n",
    "        predictions = -model.predict(X_test_processed)\n",
    "\n",
    "        print(\"Calculating Concordance Index...\")\n",
    "        print(\n",
    "            f\"Concordance index: {get_concordance_index(test_references_df, predictions)}\"\n",
    "        )\n",
    "\n",
    "    except (ConvergenceError, ValueError) as e:\n",
    "        print(\n",
    "            f\"\\nERROR: The model for {dataset_path} failed to train. No prediction file will be created.\"\n",
    "        )\n",
    "        print(f\"Details: {e}\")\n",
    "\n",
    "print(\"\\n\\nAll prediction files have been generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finsur2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

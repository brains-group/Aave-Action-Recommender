{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "# Install required packages\n",
    "# pip install -q pandas xgboost scikit-learn numpy pyreadr\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Tuple, Optional\n",
    "import pickle as pkl\n",
    "from itertools import chain\n",
    "import pyreadr\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data/\"\n",
    "CACHE_DIR = \"./cache/\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "MODEL_CACHE_DIR = os.path.join(CACHE_DIR, \"models\")\n",
    "os.makedirs(MODEL_CACHE_DIR, exist_ok=True)\n",
    "DATA_CACHE_DIR = os.path.join(CACHE_DIR, \"data\")\n",
    "os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
    "RESULTS_CACHE_DIR = os.path.join(CACHE_DIR, \"results\")\n",
    "os.makedirs(RESULTS_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "EVENTS = [\"Deposit\", \"Withdraw\", \"Repay\", \"Borrow\", \"Liquidated\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    train_df_with_labels: Optional[pd.DataFrame] = None,\n",
    "    test_features_df: Optional[pd.DataFrame] = None,\n",
    "    model_date: Optional[int] = None,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "\n",
    "    # Create unique prefix for saving/loading preprocessing objects\n",
    "    unique_prefix = (\n",
    "        (\n",
    "            present_dataframe := (\n",
    "                train_df_with_labels\n",
    "                if train_df_with_labels is not None\n",
    "                else test_features_df\n",
    "            )\n",
    "        )[\"Index Event\"].iloc[0]\n",
    "        + \"_\"\n",
    "        + present_dataframe[\"Outcome Event\"].iloc[0]\n",
    "        + (f\"_{model_date}_\" if model_date is not None else \"_\")\n",
    "    )\n",
    "    # Define paths for saving/loading preprocessing objects\n",
    "    scaler_path = os.path.join(DATA_CACHE_DIR, unique_prefix + \"scaler.pkl\")\n",
    "    train_cols = os.path.join(DATA_CACHE_DIR, unique_prefix + \"train_cols.pkl\")\n",
    "    top_categories_dict_path = os.path.join(\n",
    "        DATA_CACHE_DIR, unique_prefix + \"top_categories_dict.pkl\"\n",
    "    )\n",
    "\n",
    "    target_columns = [\"timeDiff\", \"status\"]\n",
    "    cols_to_drop = [\n",
    "        \"id\",\n",
    "        \"user\",\n",
    "        \"pool\",\n",
    "        \"Index Event\",\n",
    "        \"Outcome Event\",\n",
    "        \"type\",\n",
    "        \"timestamp\",\n",
    "    ]\n",
    "\n",
    "    if train_df_with_labels is not None:\n",
    "        if model_date is not None:\n",
    "            # model_date may be a pandas.Timestamp while dataframe timestamps are numeric.\n",
    "            # Convert model_date to numeric epoch seconds for a safe comparison.\n",
    "            if isinstance(model_date, pd.Timestamp):\n",
    "                model_date_value = model_date.timestamp() // 1e9\n",
    "            else:\n",
    "                try:\n",
    "                    model_date_value = float(model_date)\n",
    "                except Exception:\n",
    "                    model_date_value = model_date\n",
    "\n",
    "            train_df_with_labels = train_df_with_labels[\n",
    "                (train_df_with_labels[\"timestamp\"] + train_df_with_labels[\"timeDiff\"])\n",
    "                <= model_date_value\n",
    "            ]\n",
    "\n",
    "        # Separate features and targets (and drop unneeded columns from features)\n",
    "        train_targets = train_df_with_labels[target_columns]\n",
    "        train_features = train_df_with_labels.drop(\n",
    "            columns=target_columns + cols_to_drop, errors=\"ignore\"\n",
    "        )\n",
    "\n",
    "        # Make uncommon categories \"Other\" and one-hot encode categorical features\n",
    "        categorical_cols = train_features.select_dtypes(\n",
    "            include=[\"object\", \"category\"]\n",
    "        ).columns\n",
    "        top_categories_dict = {}\n",
    "        for col in categorical_cols:\n",
    "            if col not in top_categories_dict:\n",
    "                top_categories_dict[col] = (\n",
    "                    train_features[col].value_counts().nlargest(10).index\n",
    "                )\n",
    "            train_features[col] = train_features[col].where(\n",
    "                train_features[col].isin(top_categories_dict[col]), \"Other\"\n",
    "            )\n",
    "        train_features_encoded = pd.get_dummies(\n",
    "            train_features, columns=categorical_cols, dummy_na=True, drop_first=True\n",
    "        )\n",
    "\n",
    "        # Standardize numerical features\n",
    "        numerical_cols = train_features_encoded.select_dtypes(include=np.number).columns\n",
    "        scaler = StandardScaler()\n",
    "        train_features_scaled = scaler.fit_transform(\n",
    "            train_features_encoded[numerical_cols]\n",
    "        )\n",
    "        train_features_final = pd.DataFrame(\n",
    "            train_features_scaled,\n",
    "            index=train_features_encoded.index,\n",
    "            columns=numerical_cols,\n",
    "        ).fillna(0)\n",
    "\n",
    "        # Remove zero-variance columns\n",
    "        cols_to_keep = train_features_final.columns[train_features_final.var() != 0]\n",
    "        train_features_final = train_features_final[cols_to_keep]\n",
    "\n",
    "        # Save preprocessing objects\n",
    "        with open(scaler_path, \"wb\") as f:\n",
    "            pkl.dump(scaler, f)\n",
    "        with open(train_cols, \"wb\") as f:\n",
    "            pkl.dump(train_features_encoded.columns, f)\n",
    "        with open(top_categories_dict_path, \"wb\") as f:\n",
    "            pkl.dump(top_categories_dict, f)\n",
    "\n",
    "    # Process test features if provided\n",
    "    test_processed_features = None\n",
    "    if test_features_df is not None:\n",
    "        test_features = test_features_df.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "        with open(top_categories_dict_path, \"rb\") as f:\n",
    "            top_categories_dict = pkl.load(f)\n",
    "        for col in categorical_cols:\n",
    "            top_categories = top_categories_dict[col]\n",
    "            test_features[col] = test_features[col].where(\n",
    "                test_features[col].isin(top_categories), \"Other\"\n",
    "            )\n",
    "        test_features_encoded = pd.get_dummies(\n",
    "            test_features, columns=categorical_cols, dummy_na=True, drop_first=True\n",
    "        )\n",
    "        with open(train_cols, \"rb\") as f:\n",
    "            train_cols = pkl.load(f)\n",
    "        test_features_aligned = test_features_encoded.reindex(\n",
    "            columns=train_cols, fill_value=0\n",
    "        )\n",
    "        with open(scaler_path, \"rb\") as f:\n",
    "            scaler = pkl.load(f)\n",
    "        test_features_scaled = scaler.transform(test_features_aligned[numerical_cols])\n",
    "        test_features_final = pd.DataFrame(\n",
    "            test_features_scaled,\n",
    "            index=test_features_aligned.index,\n",
    "            columns=numerical_cols,\n",
    "        ).fillna(0)\n",
    "        test_processed_features = test_features_final[cols_to_keep]\n",
    "    return train_features_final, train_targets, test_processed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_for_pair_and_date(\n",
    "    index_event: str,\n",
    "    outcome_event: str,\n",
    "    model_date: int | None = None,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    # normalize model_date for filename\n",
    "    model_date_str = str(model_date) if model_date is not None else \"latest\"\n",
    "    model_filename = f\"xgboost_cox_{index_event}_{outcome_event}_{model_date_str}.ubj\"\n",
    "    model_path = os.path.join(MODEL_CACHE_DIR, model_filename)\n",
    "\n",
    "    # Create model with Cox objective\n",
    "    model = XGBRegressor(\n",
    "        objective=\"survival:cox\",\n",
    "        eval_metric=\"cox-nloglik\",\n",
    "        tree_method=\"hist\",\n",
    "        predictor=\"gpu_predictor\",\n",
    "        device=\"cuda\",\n",
    "        seed=42,\n",
    "        verbosity=0,\n",
    "        max_bin=64,\n",
    "        learning_rate=0.04,\n",
    "        max_depth=5,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.8,\n",
    "        min_child_weight=5,\n",
    "        reg_lambda=1.0,\n",
    "        reg_alpha=0.1,\n",
    "    )\n",
    "\n",
    "    # If model file exists, try to load into the estimator and return the estimator\n",
    "    if os.path.exists(model_path):\n",
    "        if verbose:\n",
    "            print(f\"Loading existing model from {model_path}\")\n",
    "        try:\n",
    "            model.load_model(model_path)\n",
    "            if verbose:\n",
    "                print(f\"model loaded from {model_path}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Warning: failed to load model from {model_path}: {e}. Will retrain.\"\n",
    "            )\n",
    "\n",
    "    dataset_path = os.path.join(index_event, outcome_event)\n",
    "\n",
    "    # --- Load and Preprocess ---\n",
    "    if verbose:\n",
    "        print(f\"Loading data from {os.path.join(DATA_PATH, dataset_path, 'data.csv')}\")\n",
    "    train_df = pd.read_csv(os.path.join(DATA_PATH, dataset_path, \"data.csv\"))\n",
    "\n",
    "    X_train, y_train, _ = preprocess(train_df, model_date=model_date)\n",
    "\n",
    "    # --- Train Model ---\n",
    "    # Prepare target variables for Cox regression\n",
    "    y_train_duration = y_train[\"timeDiff\"].values\n",
    "    y_train_event = y_train[\"status\"].values\n",
    "\n",
    "    # Fit model: XGBoost Cox expects labels to be the event indicators\n",
    "    # and the sample_weight to be the durations\n",
    "    if verbose:\n",
    "        print(\"Training model...\")\n",
    "    try:\n",
    "        model.fit(X_train, y_train_event, sample_weight=y_train_duration)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Model training failed for {dataset_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Save model: try estimator's save_model, fall back to Booster.save_model\n",
    "    try:\n",
    "        # XGBRegressor implements save_model; call it and confirm file created\n",
    "        model.save_model(model_path)\n",
    "        if verbose:\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            booster = model.get_booster()\n",
    "            booster.save_model(model_path)\n",
    "            if verbose:\n",
    "                print(f\"Model booster saved to {model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to save model to {model_path}: {e}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_for_all_event_pairs(\n",
    "    model_date: int | None = None, verbose: bool = False\n",
    "):\n",
    "    # Define all 16 event pairs\n",
    "    index_events = EVENTS\n",
    "    outcome_events = index_events\n",
    "    event_pairs = [\n",
    "        event_pair\n",
    "        for sub_event_pairs in [\n",
    "            [(index_event, outcome_event) for outcome_event in outcome_events]\n",
    "            for index_event in index_events\n",
    "        ]\n",
    "        for event_pair in sub_event_pairs\n",
    "    ]\n",
    "\n",
    "    for index_event, outcome_event in event_pairs:\n",
    "        if index_event == outcome_event and index_event == \"Liquidated\":\n",
    "            continue\n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Training for: {index_event} -> {outcome_event}\")\n",
    "            print(f\"{'='*50}\")\n",
    "\n",
    "        get_model_for_pair_and_date(\n",
    "            index_event, outcome_event, model_date=model_date, verbose=verbose\n",
    "        )\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n\\nAll prediction files have been generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_ranges():\n",
    "    if os.path.exists(os.path.join(CACHE_DIR, \"date_ranges.pkl\")):\n",
    "        with open(os.path.join(CACHE_DIR, \"date_ranges.pkl\"), \"rb\") as f:\n",
    "            return pkl.load(f)\n",
    "    transactions_df = pyreadr.read_r(\"./data/transactions.rds\")[None]\n",
    "    min_date = transactions_df[\"timestamp\"].min() * 1e9\n",
    "    print(min_date)\n",
    "    max_date = transactions_df[\"timestamp\"].max() * 1e9\n",
    "    print(max_date)\n",
    "    train_start_date = min_date + 0.4 * (max_date - min_date)\n",
    "    print(train_start_date)\n",
    "    test_start_date = min_date + 0.8 * (max_date - min_date)\n",
    "    print(test_start_date)\n",
    "    train_dates = pd.date_range(start=train_start_date, end=test_start_date, freq=\"2W\")\n",
    "    test_dates = pd.date_range(start=test_start_date, end=max_date, freq=\"2W\")\n",
    "    with open(os.path.join(CACHE_DIR, \"date_ranges.pkl\"), \"wb\") as f:\n",
    "        pkl.dump((train_dates, test_dates), f)\n",
    "    return train_dates, test_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for date in chain(*get_date_ranges()):\n",
    "#     train_models_for_all_event_pairs(model_date=date.timestamp(), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_history(user_id: str, up_to_timestamp: int) -> pd.DataFrame:\n",
    "    all_events = []\n",
    "    for index_event in EVENTS:\n",
    "        for outcome_event in EVENTS:\n",
    "            event_path = os.path.join(DATA_PATH, index_event, outcome_event, \"data.csv\")\n",
    "            if os.path.exists(event_path):\n",
    "                event_df = pd.read_csv(event_path)\n",
    "                user_events = event_df[\n",
    "                    (event_df[\"user\"] == user_id)\n",
    "                    & (event_df[\"timestamp\"] <= up_to_timestamp)\n",
    "                ]\n",
    "                if len(user_events):\n",
    "                    all_events.append(user_events)\n",
    "    if all_events:\n",
    "        user_history_df = pd.concat(all_events).sort_values(by=\"timestamp\")\n",
    "    else:\n",
    "        user_history_df = pd.DataFrame()\n",
    "    return user_history_df\n",
    "\n",
    "\n",
    "def get_transaction_history_predictions(row: pd.Series) -> pd.DataFrame:\n",
    "    results_cache_file = (\n",
    "        RESULTS_CACHE_DIR + f\"{row['user']}_{row['timestamp']}_{row['amount']}.pkl\"\n",
    "    )\n",
    "    if os.path.exists(results_cache_file):\n",
    "        with open(results_cache_file, \"rb\") as f:\n",
    "            return pkl.load(f)\n",
    "\n",
    "    results = {}\n",
    "    train_dates, test_dates = get_date_ranges()\n",
    "    dates = train_dates.union(test_dates)\n",
    "\n",
    "    user_history = get_user_history(\n",
    "        user_id=row[\"user\"], up_to_timestamp=row[\"timestamp\"]\n",
    "    )\n",
    "\n",
    "    model_date = dates[dates <= pd.to_datetime(row[\"timestamp\"], unit=\"s\")].max()\n",
    "    for _, history_row in user_history.iterrows():\n",
    "        history_timestamp = history_row[\"timestamp\"]\n",
    "        results[history_timestamp] = {}\n",
    "        history_row = history_row.copy()\n",
    "\n",
    "        index_event = history_row[\"Index Event\"].title()\n",
    "        for outcome_event in [\n",
    "            \"Liquidated\",\n",
    "            \"Borrow\",\n",
    "            \"Deposit\",\n",
    "            \"Repay\",\n",
    "            \"Withdraw\",\n",
    "        ]:\n",
    "            model = get_model_for_pair_and_date(\n",
    "                index_event, outcome_event, model_date=model_date, verbose=True\n",
    "            )\n",
    "\n",
    "            if model is None:\n",
    "                results[history_timestamp][outcome_event] = None\n",
    "                continue\n",
    "\n",
    "            history_row[\"Outcome Event\"] = outcome_event.lower()\n",
    "            _, _, test_features = preprocess(\n",
    "                test_features_df=history_row.to_frame().T,\n",
    "                model_date=model_date,\n",
    "            )\n",
    "\n",
    "            prediction = model.predict(test_features)\n",
    "            results[history_timestamp][outcome_event] = prediction[0]\n",
    "\n",
    "    with open(\n",
    "        results_cache_file,\n",
    "        \"wb\",\n",
    "    ) as f:\n",
    "        pkl.dump(results, f)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trend_slope(data):\n",
    "    \"\"\"\n",
    "    Calculates the linear regression slope of a dataset to determine the trend.\n",
    "\n",
    "    Args:\n",
    "        data (dict): A dictionary where keys are timestamps (int/float)\n",
    "                     and values are numbers.\n",
    "\n",
    "    Returns:\n",
    "        float: The slope of the trend line.\n",
    "               > 0 means increasing, < 0 means decreasing.\n",
    "               Returns 0.0 if not enough data.\n",
    "    \"\"\"\n",
    "    if len(data) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    # 1. Sort data by timestamp (keys)\n",
    "    sorted_items = sorted(data.items())\n",
    "\n",
    "    # 2. Extract x (timestamps) and y (values)\n",
    "    # We subtract the first timestamp from all x values to normalize them\n",
    "    # (starts at time 0). This prevents precision errors with large timestamps.\n",
    "    start_time = sorted_items[0][0]\n",
    "    xs = [x - start_time for x, _ in sorted_items]\n",
    "    ys = [y for _, y in sorted_items]\n",
    "\n",
    "    # 3. Calculate means\n",
    "    n = len(xs)\n",
    "    mean_x = sum(xs) / n\n",
    "    mean_y = sum(ys) / n\n",
    "\n",
    "    # 4. Calculate Slope (m) using Least Squares method\n",
    "    # Formula: m = sum((x - mean_x) * (y - mean_y)) / sum((x - mean_x)^2)\n",
    "    numerator = sum((xi - mean_x) * (yi - mean_y) for xi, yi in zip(xs, ys))\n",
    "    denominator = sum((xi - mean_x) ** 2 for xi in xs)\n",
    "\n",
    "    if denominator == 0:\n",
    "        return 0.0  # Vertical line (all timestamps are the same)\n",
    "\n",
    "    slope = numerator / denominator\n",
    "    return slope\n",
    "\n",
    "\n",
    "def determine_liquidation_risk(row: pd.Series):\n",
    "    predict_transaction_history = {\n",
    "        key: value\n",
    "        for key, value in get_transaction_history_predictions(row).items()\n",
    "        if value\n",
    "    }\n",
    "\n",
    "    is_at_risk = False\n",
    "\n",
    "    most_recent_predictions = predict_transaction_history[\n",
    "        max(predict_transaction_history.keys())\n",
    "    ]\n",
    "    if most_recent_predictions[\"Liquidated\"] >= max(most_recent_predictions.values()):\n",
    "        is_at_risk = True\n",
    "    else:\n",
    "        trend_slopes = {\n",
    "            outcome_event: calculate_trend_slope(\n",
    "                {\n",
    "                    timestamp: preds[outcome_event]\n",
    "                    for timestamp, preds in predict_transaction_history.items()\n",
    "                    if preds and outcome_event in preds\n",
    "                }\n",
    "            )\n",
    "            for outcome_event in predict_transaction_history[-1].keys()\n",
    "        }\n",
    "        if trend_slopes[\"Liquidated\"] >= max(trend_slopes.values()):\n",
    "            is_at_risk = True\n",
    "\n",
    "    return is_at_risk, most_recent_predictions, trend_slopes\n",
    "\n",
    "\n",
    "def optimize_recommendation(row: pd.Series, recommended_action: str):\n",
    "    new_action = row.copy()\n",
    "    new_action[\"timestamp\"] += 600  # Add 10 minute buffer\n",
    "    new_action[\"amount\"] = 10\n",
    "    while determine_liquidation_risk(new_action)[0]:\n",
    "        new_action[\"amount\"] *= 2\n",
    "        print(new_action[\"amount\"])\n",
    "    return new_action\n",
    "\n",
    "\n",
    "def recommend_action(row: pd.Series):\n",
    "    \"\"\"Analyze predicted transaction history to determine\n",
    "    whether the user is currently at risk of liquidation and\n",
    "    provide a simple recommended action. Returns a dictionary\n",
    "    with keys: liquidation_risk, is_at_risk, risk_trend,\n",
    "    recommended_action, reason, details.\"\"\"\n",
    "\n",
    "    is_at_risk, most_recent_predictions, _ = determine_liquidation_risk(row)\n",
    "\n",
    "    recommended_action = (\n",
    "        \"Repay\"\n",
    "        if most_recent_predictions[\"Repay\"] >= most_recent_predictions[\"Deposit\"]\n",
    "        and is_at_risk\n",
    "        else \"Deposit\"\n",
    "    )\n",
    "\n",
    "    return optimize_recommendation(row, recommended_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing model from ./cache/models/xgboost_cox_Deposit_Liquidated_2024-09-22 15:33:51.800000.ubj\n",
      "model loaded from ./cache/models/xgboost_cox_Deposit_Liquidated_2024-09-22 15:33:51.800000.ubj\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './cache/data/deposit_liquidated_2024-09-22 15:33:51.800000_top_categories_dict.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(recommendation_cache_file, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     53\u001b[39m         json.dump(recommendations, f)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43mrun_training_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mrun_training_pipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     46\u001b[39m train_set = get_train_set()\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m train_set.iterrows():\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     recommendations[row] = \u001b[43mrecommend_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     50\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(recommendation_cache_file, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mrecommend_action\u001b[39m\u001b[34m(row)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrecommend_action\u001b[39m(row: pd.Series):\n\u001b[32m     86\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Analyze predicted transaction history to determine\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[33;03m    whether the user is currently at risk of liquidation and\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[33;03m    provide a simple recommended action. Returns a dictionary\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[33;03m    with keys: liquidation_risk, is_at_risk, risk_trend,\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[33;03m    recommended_action, reason, details.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     is_at_risk, most_recent_predictions, _ = \u001b[43mdetermine_liquidation_risk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m     recommended_action = (\n\u001b[32m     95\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepay\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     96\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m most_recent_predictions[\u001b[33m\"\u001b[39m\u001b[33mRepay\u001b[39m\u001b[33m\"\u001b[39m] >= most_recent_predictions[\u001b[33m\"\u001b[39m\u001b[33mDeposit\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     97\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m is_at_risk\n\u001b[32m     98\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mDeposit\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     99\u001b[39m     )\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m optimize_recommendation(row, recommended_action)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mdetermine_liquidation_risk\u001b[39m\u001b[34m(row)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdetermine_liquidation_risk\u001b[39m(row: pd.Series):\n\u001b[32m     45\u001b[39m     predict_transaction_history = {\n\u001b[32m     46\u001b[39m         key: value\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[43mget_transaction_history_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m.items()\n\u001b[32m     48\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m value\n\u001b[32m     49\u001b[39m     }\n\u001b[32m     51\u001b[39m     is_at_risk = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     53\u001b[39m     most_recent_predictions = predict_transaction_history[\n\u001b[32m     54\u001b[39m         \u001b[38;5;28mmax\u001b[39m(predict_transaction_history.keys())\n\u001b[32m     55\u001b[39m     ]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mget_transaction_history_predictions\u001b[39m\u001b[34m(row)\u001b[39m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     59\u001b[39m history_row[\u001b[33m\"\u001b[39m\u001b[33mOutcome Event\u001b[39m\u001b[33m\"\u001b[39m] = outcome_event.lower()\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m _, _, test_features = \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_features_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory_row\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m prediction = model.predict(test_features)\n\u001b[32m     66\u001b[39m results[history_timestamp][outcome_event] = prediction[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 106\u001b[39m, in \u001b[36mpreprocess\u001b[39m\u001b[34m(train_df_with_labels, test_features_df, model_date)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m test_features_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    105\u001b[39m     test_features = test_features_df.drop(columns=cols_to_drop, errors=\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtop_categories_dict_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    107\u001b[39m         top_categories_dict = pkl.load(f)\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m categorical_cols:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/actionAgent/lib/python3.14/site-packages/IPython/core/interactiveshell.py:344\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    339\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    342\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './cache/data/deposit_liquidated_2024-09-22 15:33:51.800000_top_categories_dict.pkl'"
     ]
    }
   ],
   "source": [
    "def get_train_set():\n",
    "    train_set_dir = os.path.join(CACHE_DIR, \"train_set.csv\")\n",
    "    if os.path.exists(train_set_dir):\n",
    "        train_set = pd.read_csv(train_set_dir)\n",
    "    else:\n",
    "        train_set = pd.DataFrame()\n",
    "        train_ranges, test_ranges = get_date_ranges()\n",
    "        min_train_date = train_ranges[0].timestamp()\n",
    "        max_train_date = test_ranges[0].timestamp()\n",
    "        for index_event in EVENTS:\n",
    "            if index_event == \"Liquidated\":\n",
    "                continue\n",
    "            for outcome_event in EVENTS:\n",
    "                with open(\n",
    "                    os.path.join(\n",
    "                        DATA_PATH,\n",
    "                        index_event,\n",
    "                        outcome_event,\n",
    "                        \"data.csv\",\n",
    "                    ),\n",
    "                    \"r\",\n",
    "                ) as f:\n",
    "                    df = pd.read_csv(f)\n",
    "                    train_set = pd.concat(\n",
    "                        [\n",
    "                            train_set,\n",
    "                            df[\n",
    "                                (df[\"timestamp\"] >= min_train_date)\n",
    "                                & (df[\"timestamp\"] < max_train_date)\n",
    "                            ].sample(n=100, random_state=seed),\n",
    "                        ],\n",
    "                        ignore_index=True,\n",
    "                    )\n",
    "        with open(train_set_dir, \"w\") as f:\n",
    "            train_set.to_csv(f, index=False)\n",
    "    return train_set\n",
    "\n",
    "\n",
    "def run_training_pipeline():\n",
    "    recommendation_cache_file = os.path.join(CACHE_DIR, \"recommendations.json\")\n",
    "    if os.path.exists(recommendation_cache_file):\n",
    "        with open(recommendation_cache_file, \"r\") as f:\n",
    "            recommendations = json.load(f)\n",
    "    else:\n",
    "        recommendations = {}\n",
    "    train_set = get_train_set()\n",
    "    for i, row in train_set.iterrows():\n",
    "        recommendations[row] = recommend_action(row)\n",
    "        if i % 10 == 0:\n",
    "            with open(recommendation_cache_file, \"w\") as f:\n",
    "                json.dump(recommendations, f)\n",
    "    with open(recommendation_cache_file, \"w\") as f:\n",
    "        json.dump(recommendations, f)\n",
    "\n",
    "\n",
    "run_training_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated ./data/Deposit/Deposit/data.csv\n",
      "Updated ./data/Deposit/Withdraw/data.csv\n",
      "Updated ./data/Deposit/Repay/data.csv\n"
     ]
    }
   ],
   "source": [
    "for index_event in EVENTS:\n",
    "    for outcome_event in EVENTS:\n",
    "        csv_path = os.path.join(DATA_PATH, index_event, outcome_event, \"data.csv\")\n",
    "        if not os.path.exists(csv_path):\n",
    "            continue\n",
    "        df = pd.read_csv(csv_path)\n",
    "        for col in [\"Index Event\", \"Outcome Event\"]:\n",
    "            if col in df.columns:\n",
    "                mask = df[col].notna()\n",
    "                df.loc[mask, col] = (\n",
    "                    df.loc[mask, col]\n",
    "                    .astype(str)\n",
    "                    .str.replace(\"account liquidated\", \"liquidated\", case=False, regex=False)\n",
    "                    .str.strip()\n",
    "                )\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"Updated {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = pyreadr.read_r(\"./data/transactions.rds\")[None]\n",
    "transactions_df.to_csv(\"./data/transactions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENTS = [\"Deposit\", \"Withdraw\", \"Repay\", \"Borrow\", \"Liquidated\"]\n",
    "\n",
    "users = {}\n",
    "for index_event in EVENTS:\n",
    "    for outcome_event in EVENTS:\n",
    "        csv_path = os.path.join(DATA_PATH, index_event, outcome_event, \"data.csv\")\n",
    "        if not os.path.exists(csv_path):\n",
    "            continue\n",
    "        df = pd.read_csv(csv_path)\n",
    "        for _, row in df.iterrows():\n",
    "            if row[\"user\"] not in users:\n",
    "                users[row[\"user\"]] = {\n",
    "                    \"user_address\": \"0xUser123\",\n",
    "                    \"description\": \"My custom user profile\",\n",
    "                    \"transactions\": [],\n",
    "                }\n",
    "            users[row[\"user\"]][\"transactions\"].append(\n",
    "                {\"action\": row[\"\"], \"symbol\": \"USDC\", \"amount\": 5000, \"timestamp\": 0}\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "actionAgent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FinSurvival Competition: Starter Notebook (AFT Model Prediction Submission)\n",
    "\n",
    "**Objective:** This notebook provides a workflow for creating a valid prediction submission using the `WeibullAFTFitter` model. The competition requires you to submit a `.zip` file containing 16 separate prediction files in CSV format.\n",
    "\n",
    "This notebook will guide you through:\n",
    "1.  Loading the training and test sets for each of the 16 tasks from a single directory.\n",
    "2.  Training a model (using `WeibullAFTFitter` as an example).\n",
    "3.  Generating predictions on the test set in the required format.\n",
    "4.  Saving each set of predictions to a correctly named CSV file.\n",
    "5.  Zipping all 16 prediction files for submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# pip install -q pandas lifelines==0.27.8 scikit-learn==1.2.2 scikit-survival==0.21.0 numpy\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from lifelines import WeibullAFTFitter\n",
    "from lifelines.exceptions import ConvergenceError\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Tuple, Optional\n",
    "from utils.constants import *\n",
    "import pickle as pkl\n",
    "from xgboost import XGBRegressor\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "\n",
    "# Module-level cache for loaded/trained models to reuse across calls\n",
    "MODELS_CACHE: dict = {}\n",
    "\n",
    "# Module-level cache for preprocessing artifacts (scaler, columns, categories)\n",
    "PREPROCESS_CACHE: dict = {}\n",
    "\n",
    "def get_concordance_index(\n",
    "    test_df: pd.DataFrame, \n",
    "    predictions: np.ndarray\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculates the concordance index for survival models using scikit-survival.\n",
    "    Replaces any NaN predictions with -1.\n",
    "    \"\"\"\n",
    "    # Replace NaN predictions with a value representing the worst possible score (shortest survival)\n",
    "    # Using -1 is a robust way to handle failed predictions without causing numerical errors.\n",
    "    predictions[np.isnan(predictions)] = -1\n",
    "    \n",
    "    event_indicator = test_df['status'].astype(bool)\n",
    "    event_time = test_df['timeDiff']\n",
    "\n",
    "    # Handle cases where all events are censored or all are non-censored in the test set\n",
    "    if len(np.unique(event_indicator)) == 1:\n",
    "        return 0.5  # Return a neutral score\n",
    "\n",
    "    c_index, _, _, _, _ = concordance_index_censored(\n",
    "        event_indicator, event_time, -predictions\n",
    "    )\n",
    "    \n",
    "    return c_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define a Preprocessing Function\n",
    "\n",
    "Even though you are not submitting this code, you will still need a preprocessing pipeline to train your models effectively. You can use the one below as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    train_df_with_labels: Optional[pd.DataFrame] = None,\n",
    "    test_features_df: Optional[pd.DataFrame] = None,\n",
    "    model_date: Optional[int] = None,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "\n",
    "    # Create unique prefix for saving/loading preprocessing objects\n",
    "    unique_prefix = (\n",
    "        (\n",
    "            present_dataframe := (\n",
    "                train_df_with_labels\n",
    "                if train_df_with_labels is not None\n",
    "                else test_features_df\n",
    "            )\n",
    "        )[\"Index Event\"].iloc[0]\n",
    "        + \"_\"\n",
    "        + present_dataframe[\"Outcome Event\"].iloc[0]\n",
    "        + (f\"_{model_date}_\" if model_date is not None else \"_\")\n",
    "    )\n",
    "    # Define paths for saving/loading preprocessing objects\n",
    "    scaler_path = os.path.join(DATA_CACHE_DIR, unique_prefix + \"scaler.pkl\")\n",
    "    train_cols = os.path.join(DATA_CACHE_DIR, unique_prefix + \"train_cols.pkl\")\n",
    "    top_categories_dict_path = os.path.join(\n",
    "        DATA_CACHE_DIR, unique_prefix + \"top_categories_dict.pkl\"\n",
    "    )\n",
    "    categorical_cols_path = os.path.join(\n",
    "        DATA_CACHE_DIR, unique_prefix + \"categorical_cols.pkl\"\n",
    "    )\n",
    "    numerical_cols_path = os.path.join(\n",
    "        DATA_CACHE_DIR, unique_prefix + \"numerical_cols.pkl\"\n",
    "    )\n",
    "    cols_to_keep_path = os.path.join(DATA_CACHE_DIR, unique_prefix + \"cols_to_keep.pkl\")\n",
    "\n",
    "    target_columns = [\"timeDiff\", \"status\"]\n",
    "    cols_to_drop = [\n",
    "        \"id\",\n",
    "        \"user\",\n",
    "        \"pool\",\n",
    "        \"Index Event\",\n",
    "        \"Outcome Event\",\n",
    "        \"type\",\n",
    "        \"timestamp\",\n",
    "    ]\n",
    "\n",
    "    if train_df_with_labels is not None:\n",
    "        if model_date is not None:\n",
    "            # model_date may be a pandas.Timestamp while dataframe timestamps are numeric.\n",
    "            # Convert model_date to numeric epoch seconds for a safe comparison.\n",
    "            if isinstance(model_date, pd.Timestamp):\n",
    "                model_date_value = model_date.timestamp()\n",
    "            else:\n",
    "                try:\n",
    "                    model_date_value = float(model_date)\n",
    "                except Exception:\n",
    "                    model_date_value = model_date\n",
    "\n",
    "            train_df_with_labels = train_df_with_labels[\n",
    "                (train_df_with_labels[\"timestamp\"] + train_df_with_labels[\"timeDiff\"])\n",
    "                <= model_date_value\n",
    "            ]\n",
    "\n",
    "        # Separate features and targets (and drop unneeded columns from features)\n",
    "        train_targets = train_df_with_labels[target_columns]\n",
    "        train_features = train_df_with_labels.drop(\n",
    "            columns=target_columns + cols_to_drop, errors=\"ignore\"\n",
    "        )\n",
    "\n",
    "        # Make uncommon categories \"Other\" and one-hot encode categorical features\n",
    "        categorical_cols = train_features.select_dtypes(\n",
    "            include=[\"object\", \"category\"]\n",
    "        ).columns\n",
    "        top_categories_dict = {}\n",
    "        for col in categorical_cols:\n",
    "            top_categories_dict[col] = (\n",
    "                train_features[col].value_counts().nlargest(10).index\n",
    "            )\n",
    "            train_features[col] = train_features[col].where(\n",
    "                train_features[col].isin(top_categories_dict[col]), \"Other\"\n",
    "            )\n",
    "        train_features_encoded = pd.get_dummies(\n",
    "            train_features, columns=categorical_cols, dummy_na=True, drop_first=True\n",
    "        )\n",
    "\n",
    "        # Standardize numerical features\n",
    "        numerical_cols = train_features_encoded.select_dtypes(include=np.number).columns\n",
    "        scaler = StandardScaler()\n",
    "        train_features_scaled = scaler.fit_transform(\n",
    "            train_features_encoded[numerical_cols]\n",
    "        )\n",
    "        train_features_final = pd.DataFrame(\n",
    "            train_features_scaled,\n",
    "            index=train_features_encoded.index,\n",
    "            columns=numerical_cols,\n",
    "        ).fillna(0)\n",
    "\n",
    "        # Remove zero-variance columns\n",
    "        cols_to_keep = train_features_final.columns[train_features_final.var() != 0]\n",
    "        train_features_final = train_features_final[cols_to_keep]\n",
    "\n",
    "        # Save preprocessing objects\n",
    "        with open(scaler_path, \"wb\") as f:\n",
    "            pkl.dump(scaler, f)\n",
    "        with open(train_cols, \"wb\") as f:\n",
    "            pkl.dump(train_features_encoded.columns, f)\n",
    "        with open(top_categories_dict_path, \"wb\") as f:\n",
    "            pkl.dump(top_categories_dict, f)\n",
    "        with open(categorical_cols_path, \"wb\") as f:\n",
    "            pkl.dump(categorical_cols, f)\n",
    "        with open(numerical_cols_path, \"wb\") as f:\n",
    "            pkl.dump(numerical_cols, f)\n",
    "        with open(cols_to_keep_path, \"wb\") as f:\n",
    "            pkl.dump(cols_to_keep, f)\n",
    "\n",
    "        # Populate in-memory preprocess cache for fast reuse\n",
    "        PREPROCESS_CACHE[unique_prefix] = {\n",
    "            \"scaler\": scaler,\n",
    "            \"train_cols\": train_features_encoded.columns,\n",
    "            \"top_categories_dict\": top_categories_dict,\n",
    "            \"categorical_cols\": categorical_cols,\n",
    "            \"numerical_cols\": numerical_cols,\n",
    "            \"cols_to_keep\": cols_to_keep,\n",
    "        }\n",
    "    else:\n",
    "        train_features_final = None\n",
    "        train_targets = None\n",
    "\n",
    "    # Process test features if provided\n",
    "    if test_features_df is not None:\n",
    "        if unique_prefix not in PREPROCESS_CACHE:\n",
    "            PREPROCESS_CACHE[unique_prefix] = {}\n",
    "            with open(categorical_cols_path, \"rb\") as f:\n",
    "                PREPROCESS_CACHE[unique_prefix][\"categorical_cols\"] = pkl.load(f)\n",
    "            with open(top_categories_dict_path, \"rb\") as f:\n",
    "                PREPROCESS_CACHE[unique_prefix][\"top_categories_dict\"] = pkl.load(f)\n",
    "            with open(numerical_cols_path, \"rb\") as f:\n",
    "                PREPROCESS_CACHE[unique_prefix][\"numerical_cols\"] = pkl.load(f)\n",
    "            with open(cols_to_keep_path, \"rb\") as f:\n",
    "                PREPROCESS_CACHE[unique_prefix][\"cols_to_keep\"] = pkl.load(f)\n",
    "            with open(train_cols, \"rb\") as f:\n",
    "                PREPROCESS_CACHE[unique_prefix][\"train_cols\"] = pkl.load(f)\n",
    "            with open(scaler_path, \"rb\") as f:\n",
    "                PREPROCESS_CACHE[unique_prefix][\"scaler\"] = pkl.load(f)\n",
    "\n",
    "        test_features = test_features_df.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "        categorical_cols = PREPROCESS_CACHE[unique_prefix][\"categorical_cols\"]\n",
    "        top_categories_dict = PREPROCESS_CACHE[unique_prefix][\"top_categories_dict\"]\n",
    "        for col in categorical_cols:\n",
    "            top_categories = top_categories_dict[col]\n",
    "            test_features[col] = test_features[col].where(\n",
    "                test_features[col].isin(top_categories), \"Other\"\n",
    "            )\n",
    "        test_features_encoded = pd.get_dummies(\n",
    "            test_features, columns=categorical_cols, dummy_na=True, drop_first=True\n",
    "        )\n",
    "        train_cols = PREPROCESS_CACHE[unique_prefix][\"train_cols\"]\n",
    "        test_features_aligned = test_features_encoded.reindex(\n",
    "            columns=train_cols, fill_value=0\n",
    "        )\n",
    "        scaler = PREPROCESS_CACHE[unique_prefix][\"scaler\"]\n",
    "        numerical_cols = PREPROCESS_CACHE[unique_prefix][\"numerical_cols\"]\n",
    "        test_features_scaled = scaler.transform(test_features_aligned[numerical_cols])\n",
    "        test_features_final = pd.DataFrame(\n",
    "            test_features_scaled,\n",
    "            index=test_features_aligned.index,\n",
    "            columns=numerical_cols,\n",
    "        ).fillna(0)\n",
    "        cols_to_keep = PREPROCESS_CACHE[unique_prefix][\"cols_to_keep\"]\n",
    "        # logger.debug(\"cols_to_keep:%s\", cols_to_keep)\n",
    "        test_processed_features = test_features_final[cols_to_keep]\n",
    "    else:\n",
    "        test_processed_features = None\n",
    "    return train_features_final, train_targets, test_processed_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Loop, Train, and Save Predictions\n",
    "\n",
    "This is the main part of the notebook. We will loop through all 16 tasks. For each task, we will:\n",
    "1. Load the training data and the test features.\n",
    "2. Preprocess both.\n",
    "3. Train a model on the training data.\n",
    "4. Generate predictions on the processed test features.\n",
    "5. Save the predictions to a CSV file with the correct name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Processing and Predicting for: Borrow -> Borrow\n",
      "==================================================\n",
      "Generating predictions for Borrow/Borrow...\n"
     ]
    }
   ],
   "source": [
    "# Define path to the single participant data folder.\n",
    "PARTICIPANT_DATA_PATH = \"./data/\"\n",
    "\n",
    "# Define all 16 event pairs\n",
    "index_events = [\"Borrow\", \"Deposit\", \"Repay\", \"Withdraw\", \"Liquidated\"]\n",
    "outcome_events = index_events\n",
    "event_pairs = [\n",
    "    (index_event, outcome_event)\n",
    "    for index_event in index_events\n",
    "    for outcome_event in outcome_events\n",
    "]\n",
    "\n",
    "for index_event, outcome_event in event_pairs:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing and Predicting for: {index_event} -> {outcome_event}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    dataset_path = os.path.join(index_event, outcome_event)\n",
    "\n",
    "    # --- Load and Preprocess ---\n",
    "    try:\n",
    "        data_df = pd.read_csv(\n",
    "            os.path.join(PARTICIPANT_DATA_PATH, dataset_path, \"data.csv\")\n",
    "        )\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Data not found for {dataset_path}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    buffer_duration = 30 * 60 * 60 * 24\n",
    "    train_cutoff = 1722526142 - buffer_duration\n",
    "\n",
    "    train_df = data_df[data_df[\"timestamp\"] <= train_cutoff]\n",
    "    test_df = data_df[data_df[\"timestamp\"] > train_cutoff]\n",
    "    reference_cols = [\"timeDiff\", \"status\"]\n",
    "    feature_cols = [col for col in train_df.columns if col not in reference_cols]\n",
    "    test_features_df = test_df[feature_cols]\n",
    "    test_references_df = test_df[reference_cols]\n",
    "\n",
    "    X_train, y_train, X_test_processed = preprocess(train_df, test_features_df)\n",
    "\n",
    "    # --- Train Model ---\n",
    "    try:\n",
    "        lifelines_train_df = pd.concat(\n",
    "            [X_train, y_train.reset_index(drop=True)], axis=1\n",
    "        )\n",
    "        lifelines_train_df = lifelines_train_df.loc[\n",
    "            lifelines_train_df[\"timeDiff\"] > 0\n",
    "        ].copy()\n",
    "\n",
    "        model = XGBRegressor(\n",
    "            objective=\"survival:cox\",\n",
    "            eval_metric=\"cox-nloglik\",\n",
    "            tree_method=\"hist\",\n",
    "            predictor=\"gpu_predictor\",\n",
    "            device=\"cuda\",\n",
    "            seed=42,\n",
    "            verbosity=0,\n",
    "            max_bin=64,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=6,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            min_child_weight=5,\n",
    "            reg_lambda=1.0,\n",
    "            reg_alpha=0.1,\n",
    "        )\n",
    "        y_train_duration = y_train[\"timeDiff\"].values\n",
    "        y_train_event = y_train[\"status\"].values\n",
    "        model.fit(X_train, y_train_event, sample_weight=y_train_duration)\n",
    "\n",
    "        # --- Generate and Save Predictions ---\n",
    "        print(f\"Generating predictions for {dataset_path}...\")\n",
    "        # Use the processed test features to make predictions\n",
    "        predictions = model.predict(X_test_processed, output_margin=True)\n",
    "\n",
    "        print(f\"Concordance index: {get_concordance_index(test_references_df, predictions)}\")\n",
    "\n",
    "    except (ConvergenceError, ValueError) as e:\n",
    "        print(\n",
    "            f\"\\nERROR: The model for {dataset_path} failed to train. No prediction file will be created.\"\n",
    "        )\n",
    "        print(f\"Details: {e}\")\n",
    "\n",
    "print(\"\\n\\nAll prediction files have been generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finsur2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
